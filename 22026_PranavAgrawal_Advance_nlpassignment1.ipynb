{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Assistant reviewing the LSTM class implementation below. This cell is just a marker for the review session; it can be removed later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Pfh84KzTpAnh"
   },
   "outputs": [],
   "source": [
    "# Core libraries and dependencies for text classification models\n",
    "import csv\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "QsV2JxW_4Asz"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "# Download training and test datasets from repository\n",
    "!wget https://raw.githubusercontent.com/pranavagrawaI/anlp-assignment-1/main/Corona_NLP_train.csv\n",
    "!wget https://raw.githubusercontent.com/pranavagrawaI/anlp-assignment-1/main/Corona_NLP_test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "x0Qy72HPkRpk"
   },
   "outputs": [],
   "source": [
    "# Configuration and hyperparameters for all models\n",
    "TRAIN_CSV = \"Corona_NLP_train.csv\"\n",
    "TEST_CSV = \"Corona_NLP_test.csv\"\n",
    "\n",
    "TRAIN_MODE = True\n",
    "TEXT_COL = \"OriginalTweet\"\n",
    "LABEL_COL = \"Sentiment\"\n",
    "\n",
    "RNN_OUT_DIR = \"rnn_artifacts\"\n",
    "LSTM_OUT_DIR = \"lstm_artifacts\"\n",
    "TRANSFORMERS_OUT_DIR = \"transformers_artifacts\"\n",
    "MAX_LEN = 40\n",
    "EMB_DIM = 100\n",
    "HIDDEN_DIM = 128\n",
    "EPOCHS = 5\n",
    "BATCH_SIZE = 64\n",
    "LR = 1e-3\n",
    "WEIGHT_DECAY = 1e-4\n",
    "DROPOUT_P = 0.1\n",
    "EARLY_STOP = 3\n",
    "GRAD_CLIP = 1.0\n",
    "RNG_SEED = 42\n",
    "\n",
    "NUM_LAYERS = 1\n",
    "NUM_HEADS = 2\n",
    "DIM_FEEDFORWARD = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "vcoRgIIxzYxe"
   },
   "outputs": [],
   "source": [
    "# Text preprocessing, tokenization, and special token handling\n",
    "random.seed(RNG_SEED)\n",
    "np.random.seed(RNG_SEED)\n",
    "\n",
    "SPECIAL_TOKENS = [\n",
    "    \"<PAD>\",\n",
    "    \"<UNK>\",\n",
    "    \"<BOS>\",\n",
    "    \"<EOS>\",\n",
    "    \"<URL>\",\n",
    "    \"<USER>\",\n",
    "    \"<NUM>\",\n",
    "    \"<EMO_POS>\",\n",
    "    \"<EMO_NEG>\",\n",
    "]\n",
    "PAD, UNK, BOS, EOS, URL_T, USER_T, NUM_T, EMO_POS, EMO_NEG = range(len(SPECIAL_TOKENS))\n",
    "\n",
    "URL_RE = re.compile(r\"https?://\\S+|www\\.[^\\s]+\", re.I)\n",
    "USER_RE = re.compile(r\"@[A-Za-z0-9_]+\")\n",
    "NUM_RE = re.compile(r\"(?<![A-Za-z])[-+]?\\d+[\\d,\\.]*\")\n",
    "EMO_POS_RE = re.compile(r\"[ðŸ˜€ðŸ˜ƒðŸ˜„ðŸ˜ðŸ˜†ðŸ˜ŠðŸ™‚ðŸ˜ðŸ˜˜ðŸ˜ºðŸ‘â¤ï¸â™¥ï¸ðŸ’–âœ¨]\")\n",
    "EMO_NEG_RE = re.compile(r\"[ðŸ˜žðŸ˜ŸðŸ˜ ðŸ˜¡ðŸ˜¢ðŸ˜­ðŸ˜”ðŸ™ðŸ˜•ðŸ‘Ž]\")\n",
    "WS_RE = re.compile(r\"\\s+\")\n",
    "TOKEN_RE = re.compile(r\"[\\w\\-']+|[^\\w\\s]\")\n",
    "\n",
    "\n",
    "def normalize_text(s: str) -> str:\n",
    "    s = s.strip().lower()\n",
    "    s = URL_RE.sub(\" <URL> \", s)\n",
    "    s = USER_RE.sub(\" <USER> \", s)\n",
    "    s = NUM_RE.sub(\" <NUM> \", s)\n",
    "    s = EMO_POS_RE.sub(\" <EMO_POS> \", s)\n",
    "    s = EMO_NEG_RE.sub(\" <EMO_NEG> \", s)\n",
    "    s = WS_RE.sub(\" \", s)\n",
    "    return s.strip()\n",
    "\n",
    "\n",
    "def simple_tokenize(s: str):\n",
    "    return TOKEN_RE.findall(s)\n",
    "\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self, max_vocab=30000, min_freq=1):\n",
    "        self.max_vocab = max_vocab\n",
    "        self.min_freq = min_freq\n",
    "        self.token2id, self.id2token = {}, []\n",
    "\n",
    "    def fit(self, texts):\n",
    "        counter = Counter()\n",
    "        for t in texts:\n",
    "            counter.update(simple_tokenize(normalize_text(t)))\n",
    "        for sp in SPECIAL_TOKENS:\n",
    "            counter[sp] += 10**9\n",
    "        items = [w for w, c in counter.items() if c >= self.min_freq]\n",
    "        items.sort(key=lambda w: counter[w], reverse=True)\n",
    "        items = items[: self.max_vocab]\n",
    "        self.id2token = items\n",
    "        self.token2id = {w: i for i, w in enumerate(items)}\n",
    "        for i, sp in enumerate(SPECIAL_TOKENS):\n",
    "            self.token2id[sp] = i\n",
    "            self.id2token[i] = sp\n",
    "\n",
    "    def encode(self, s, add_bos_eos=True):\n",
    "        toks = simple_tokenize(normalize_text(s))\n",
    "        ids = [BOS] if add_bos_eos else []\n",
    "        for tok in toks:\n",
    "            if tok in (\"<URL>\", \"<USER>\", \"<NUM>\", \"<EMO_POS>\", \"<EMO_NEG>\"):\n",
    "                ids.append(self.token2id[tok])\n",
    "            else:\n",
    "                ids.append(self.token2id.get(tok, UNK))\n",
    "        if add_bos_eos:\n",
    "            ids.append(EOS)\n",
    "        return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "j6kxVjlA0GwY"
   },
   "outputs": [],
   "source": [
    "# Data loading and preprocessing utility functions\n",
    "def read_csv_text_label(path, text_col, label_col):\n",
    "    texts, labels = [], []\n",
    "    with open(path, newline=\"\", encoding=\"ISO-8859-1\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            t, y = row[text_col], row[label_col]\n",
    "            if t and y:\n",
    "                texts.append(str(t))\n",
    "                labels.append(str(y))\n",
    "    return texts, labels\n",
    "\n",
    "\n",
    "def pad_sequences(seqs, max_len):\n",
    "    N = len(seqs)\n",
    "    arr = np.full((N, max_len), PAD, dtype=np.int32)\n",
    "    mask = np.zeros((N, max_len), dtype=np.float32)\n",
    "    for i, s in enumerate(seqs):\n",
    "        L = min(len(s), max_len)\n",
    "        arr[i, :L] = s[:L]\n",
    "        mask[i, :L] = 1.0\n",
    "    return arr, mask\n",
    "\n",
    "\n",
    "def stratified_split(labels, val_ratio=0.15, seed=None):\n",
    "    rng = random.Random(seed) if seed is not None else random\n",
    "    label2idx = defaultdict(list)\n",
    "    for i, y in enumerate(labels):\n",
    "        label2idx[y].append(i)\n",
    "    train_idx, val_idx = [], []\n",
    "    for label in sorted(label2idx.keys()):\n",
    "        idxs = label2idx[label]\n",
    "        rng.shuffle(idxs)\n",
    "        k = max(1, int(round(len(idxs) * val_ratio)))\n",
    "        val_idx += idxs[:k]\n",
    "        train_idx += idxs[k:]\n",
    "    rng.shuffle(train_idx)\n",
    "    rng.shuffle(val_idx)\n",
    "    return train_idx, val_idx\n",
    "\n",
    "\n",
    "def build_label_map(labels):\n",
    "    uniq = sorted(set(labels))\n",
    "    label2id = {y: i for i, y in enumerate(uniq)}\n",
    "    id2label = {i: y for i, y in enumerate(uniq)}\n",
    "    return label2id, id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "1iMnTEeN0Bfc"
   },
   "outputs": [],
   "source": [
    "# Embedding layer and vanilla RNN implementation with forward/backward passes\n",
    "def orthogonal_(shape, gain=1.0):\n",
    "    a = np.random.randn(*shape).astype(np.float32)\n",
    "    u, _, v = np.linalg.svd(a, full_matrices=False)\n",
    "    q = u if u.shape == shape else v\n",
    "    return (gain * q).astype(np.float32)\n",
    "\n",
    "\n",
    "class Embedding:\n",
    "    def __init__(self, vocab_size, dim):\n",
    "        lim = 1.0 / math.sqrt(dim)\n",
    "        self.W = np.random.uniform(-lim, lim, (vocab_size, dim)).astype(np.float32)\n",
    "        self.W[PAD] = 0.0\n",
    "        self.grad = np.zeros_like(self.W)\n",
    "        self.last_idx = None\n",
    "\n",
    "    def forward(self, x_ids):\n",
    "        self.last_idx = x_ids\n",
    "        return self.W[x_ids]\n",
    "\n",
    "    def backward(self, dE):\n",
    "        self.grad.fill(0.0)\n",
    "        idx = self.last_idx\n",
    "        mask_bt1 = (idx != PAD)[..., None].astype(np.float32)\n",
    "        dE_masked = dE * mask_bt1\n",
    "        np.add.at(self.grad, idx, dE_masked)\n",
    "        self.grad[PAD] = 0.0\n",
    "\n",
    "\n",
    "class RNN:\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.Wxh = (\n",
    "            np.random.randn(input_dim, hidden_dim).astype(np.float32)\n",
    "            / np.sqrt(input_dim)\n",
    "        ) * 0.5\n",
    "        self.Whh = orthogonal_((hidden_dim, hidden_dim), gain=1.0)\n",
    "        self.bh = np.zeros((hidden_dim,), dtype=np.float32)\n",
    "        self.dWxh = np.zeros_like(self.Wxh)\n",
    "        self.dWhh = np.zeros_like(self.Whh)\n",
    "        self.dbh = np.zeros_like(self.bh)\n",
    "        self.last_x = None\n",
    "        self.last_h = None\n",
    "        self.last_mask = None\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        B, T, D = x.shape\n",
    "        H = self.hidden_dim\n",
    "        h = np.zeros((B, T, H), dtype=np.float32)\n",
    "        h_prev = np.zeros((B, H), dtype=np.float32)\n",
    "        for t in range(T):\n",
    "            xt = x[:, t, :]\n",
    "            pre = xt @ self.Wxh + h_prev @ self.Whh + self.bh\n",
    "            ht = np.tanh(pre)\n",
    "            mt = mask[:, t : t + 1]\n",
    "            ht = mt * ht + (1.0 - mt) * h_prev\n",
    "            h[:, t, :] = ht\n",
    "            h_prev = ht\n",
    "        self.last_x, self.last_h, self.last_mask = x, h, mask\n",
    "        return h\n",
    "\n",
    "    def backward(self, dh):\n",
    "        x, h, mask = self.last_x, self.last_h, self.last_mask\n",
    "        B, T, D = x.shape\n",
    "        H = self.hidden_dim\n",
    "        dWxh = np.zeros_like(self.Wxh)\n",
    "        dWhh = np.zeros_like(self.Whh)\n",
    "        dbh = np.zeros_like(self.bh)\n",
    "        dx = np.zeros_like(x)\n",
    "        dh_next = np.zeros((B, H), dtype=np.float32)\n",
    "        for t in reversed(range(T)):\n",
    "            mt = mask[:, t : t + 1]\n",
    "            ht = h[:, t, :]\n",
    "            hprev = h[:, t - 1, :] if t > 0 else np.zeros_like(ht)\n",
    "            xt = x[:, t, :]\n",
    "            dht = dh[:, t, :] + dh_next\n",
    "            dht = dht * mt\n",
    "            pre_grad = (1.0 - ht * ht) * dht\n",
    "            dWxh += xt.T @ pre_grad\n",
    "            dWhh += hprev.T @ pre_grad\n",
    "            dbh += np.sum(pre_grad, axis=0)\n",
    "            dx[:, t, :] = pre_grad @ self.Wxh.T\n",
    "            dh_next = pre_grad @ self.Whh.T\n",
    "        self.dWxh[...] = dWxh\n",
    "        self.dWhh[...] = dWhh\n",
    "        self.dbh[...] = dbh\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "J4WIDBkNJNUb"
   },
   "outputs": [],
   "source": [
    "# LSTM implementation with forward and backward passes\n",
    "class LSTM:\n",
    "    def __init__(self, input_dim, hidden_dim, seed=0):\n",
    "        rng = np.random.default_rng(seed)\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.W_ih = rng.normal(\n",
    "            0, 1 / np.sqrt(input_dim), (input_dim, 4 * hidden_dim)\n",
    "        ).astype(np.float32)\n",
    "        self.W_hh = rng.normal(\n",
    "            0, 1 / np.sqrt(hidden_dim), (hidden_dim, 4 * hidden_dim)\n",
    "        ).astype(np.float32)\n",
    "        self.b = np.zeros(4 * hidden_dim, dtype=np.float32)\n",
    "        self.b[hidden_dim : 2 * hidden_dim] = 1.0\n",
    "\n",
    "        self.dW_ih = np.zeros_like(self.W_ih)\n",
    "        self.dW_hh = np.zeros_like(self.W_hh)\n",
    "        self.db = np.zeros_like(self.b)\n",
    "\n",
    "        self.cache = None\n",
    "\n",
    "    @staticmethod\n",
    "    def _sigmoid(x):\n",
    "        pos = x >= 0\n",
    "        z = np.empty_like(x, dtype=np.float32)\n",
    "        z[pos] = 1.0 / (1.0 + np.exp(-x[pos]))\n",
    "        ex = np.exp(x[~pos])\n",
    "        z[~pos] = ex / (1.0 + ex)\n",
    "        return z.astype(np.float32)\n",
    "\n",
    "    def zero_grads(self):\n",
    "        self.dW_ih.fill(0.0)\n",
    "        self.dW_hh.fill(0.0)\n",
    "        self.db.fill(0.0)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        B, T, D = x.shape\n",
    "        H = self.hidden_dim\n",
    "        h_prev = np.zeros((B, H), dtype=np.float32)\n",
    "        c_prev = np.zeros((B, H), dtype=np.float32)\n",
    "\n",
    "        h_all = np.zeros((B, T, H), dtype=np.float32)\n",
    "        self.cache = []\n",
    "\n",
    "        for t in range(T):\n",
    "            xt = x[:, t, :]\n",
    "            mt = mask[:, t : t + 1].astype(np.float32)\n",
    "\n",
    "            gates = xt @ self.W_ih + h_prev @ self.W_hh + self.b\n",
    "            i_pre, f_pre, g_pre, o_pre = np.split(gates, 4, axis=1)\n",
    "\n",
    "            i = self._sigmoid(i_pre)\n",
    "            f = self._sigmoid(f_pre)\n",
    "            g = np.tanh(g_pre).astype(np.float32)\n",
    "            o = self._sigmoid(o_pre)\n",
    "\n",
    "            c_bar = f * c_prev + i * g\n",
    "            h_bar = o * np.tanh(c_bar).astype(np.float32)\n",
    "\n",
    "            c_next = mt * c_bar + (1.0 - mt) * c_prev\n",
    "            h_next = mt * h_bar + (1.0 - mt) * h_prev\n",
    "\n",
    "            h_all[:, t, :] = h_next\n",
    "\n",
    "            self.cache.append((xt, h_prev, c_prev, i, f, g, o, c_bar, h_bar, mt))\n",
    "\n",
    "            h_prev, c_prev = h_next, c_next\n",
    "\n",
    "        return h_all\n",
    "\n",
    "    def backward(self, dh_all):\n",
    "        assert self.cache is not None, \"call forward() first\"\n",
    "        B, T, H = dh_all.shape\n",
    "        D = self.input_dim\n",
    "\n",
    "        dx = np.zeros((B, T, D), dtype=np.float32)\n",
    "        self.zero_grads()\n",
    "\n",
    "        dh_next = np.zeros((B, H), dtype=np.float32)\n",
    "        dc_next = np.zeros((B, H), dtype=np.float32)\n",
    "\n",
    "        for t in reversed(range(T)):\n",
    "            xt, h_prev, c_prev, i, f, g, o, c_bar, h_bar, mt = self.cache[t]\n",
    "\n",
    "            dh = dh_all[:, t, :] + dh_next\n",
    "\n",
    "            dh_bar = dh * mt\n",
    "            dh_prev_mask = dh * (1.0 - mt)\n",
    "\n",
    "            tanh_c = np.tanh(c_bar).astype(np.float32)\n",
    "            do = dh_bar * tanh_c * o * (1.0 - o)\n",
    "\n",
    "            dc_bar = dh_bar * o * (1.0 - tanh_c**2) + dc_next * mt\n",
    "\n",
    "            di = dc_bar * g * i * (1.0 - i)\n",
    "            df = dc_bar * c_prev * f * (1.0 - f)\n",
    "            dg = dc_bar * i * (1.0 - g**2)\n",
    "\n",
    "            dc_prev_lstm = dc_bar * f\n",
    "\n",
    "            dgates = np.concatenate([di, df, dg, do], axis=1).astype(np.float32)\n",
    "\n",
    "            self.dW_ih += xt.T @ dgates\n",
    "            self.dW_hh += h_prev.T @ dgates\n",
    "            self.db += dgates.sum(axis=0)\n",
    "\n",
    "            dx_t = dgates @ self.W_ih.T\n",
    "            dh_prev_lstm = dgates @ self.W_hh.T\n",
    "\n",
    "            dc_prev = dc_prev_lstm + dc_next * (1.0 - mt)\n",
    "\n",
    "            dh_prev = dh_prev_lstm * mt + dh_prev_mask\n",
    "\n",
    "            dh_next = dh_prev\n",
    "            dc_next = dc_prev\n",
    "            dx[:, t, :] = dx_t\n",
    "\n",
    "        return dx\n",
    "\n",
    "    def step_sgd(self, lr=1e-2, weight_decay=0.0):\n",
    "        if weight_decay != 0.0:\n",
    "            self.dW_ih += weight_decay * self.W_ih\n",
    "            self.dW_hh += weight_decay * self.W_hh\n",
    "        self.W_ih -= lr * self.dW_ih\n",
    "        self.W_hh -= lr * self.dW_hh\n",
    "        self.b -= lr * self.db\n",
    "        self.zero_grads()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "wARQKjRqvwEz"
   },
   "outputs": [],
   "source": [
    "# Linear layer, loss functions, optimizer, and training utilities\n",
    "class Linear:\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        lim = 1.0 / math.sqrt(in_dim)\n",
    "        self.W = np.random.uniform(-lim, +lim, (in_dim, out_dim)).astype(np.float32)\n",
    "        self.b = np.zeros((out_dim,), dtype=np.float32)\n",
    "        self.dW = np.zeros_like(self.W)\n",
    "        self.db = np.zeros_like(self.b)\n",
    "        self.last_x = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.last_x = x.copy()\n",
    "        return x @ self.W + self.b\n",
    "\n",
    "    def backward(self, dY):\n",
    "        if self.last_x is None:\n",
    "            raise RuntimeError(\"backward() called before forward()\")\n",
    "\n",
    "        x = self.last_x\n",
    "        if x.shape[:-1] != dY.shape[:-1]:\n",
    "            raise ValueError(\n",
    "                f\"Shape mismatch: forward input was {x.shape} but backward gradient is {dY.shape}\"\n",
    "            )\n",
    "\n",
    "        x_flat = x.reshape(-1, x.shape[-1])\n",
    "        dY_flat = dY.reshape(-1, dY.shape[-1])\n",
    "\n",
    "        dW = x_flat.T @ dY_flat\n",
    "        db = np.sum(dY_flat, axis=0)\n",
    "\n",
    "        self.dW += dW\n",
    "        self.db += db\n",
    "\n",
    "        dx_flat = dY_flat @ self.W.T\n",
    "        dx = dx_flat.reshape(x.shape)\n",
    "        return dx\n",
    "\n",
    "    def zero_grad(self):\n",
    "        self.dW.fill(0.0)\n",
    "        self.db.fill(0.0)\n",
    "        self.last_x = None\n",
    "\n",
    "\n",
    "def weighted_cross_entropy(logits, y, w):\n",
    "    B, C = logits.shape\n",
    "    exp_shift = np.max(logits, axis=1, keepdims=True)\n",
    "    exp_logits = np.exp(logits - exp_shift)\n",
    "    Z = np.sum(exp_logits, axis=1, keepdims=True)\n",
    "    log_probs = (logits - exp_shift) - np.log(Z)\n",
    "\n",
    "    true_log_probs = log_probs[np.arange(B), y]\n",
    "    weighted_log_probs = true_log_probs * w[y]\n",
    "    loss = -np.mean(weighted_log_probs)\n",
    "\n",
    "    probs = exp_logits / Z\n",
    "    dlog = probs.copy()\n",
    "    dlog[np.arange(B), y] -= 1.0\n",
    "    dlog *= w[y, None]\n",
    "    dlog /= B\n",
    "    return float(loss), dlog\n",
    "\n",
    "\n",
    "def compute_class_weights(y, num_classes):\n",
    "    counts = np.bincount(y, minlength=num_classes).astype(np.float32)\n",
    "    counts[counts == 0] = 1.0\n",
    "    inv = 1.0 / counts\n",
    "    w = inv * (num_classes / np.sum(inv))\n",
    "    return w.astype(np.float32)\n",
    "\n",
    "\n",
    "def classification_report(y_true, y_pred, id2label):\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "    label_set = sorted(set(y_true.tolist()) | set(y_pred.tolist()))\n",
    "    lines, macro = [], 0.0\n",
    "    for c in label_set:\n",
    "        tp = np.sum((y_true == c) & (y_pred == c))\n",
    "        fp = np.sum((y_true != c) & (y_pred == c))\n",
    "        fn = np.sum((y_true == c) & (y_pred != c))\n",
    "        p = tp / (tp + fp + 1e-12)\n",
    "        r = tp / (tp + fn + 1e-12)\n",
    "        f1 = 2 * p * r / (p + r + 1e-12)\n",
    "        macro += f1\n",
    "        lines.append(f\"{id2label[c]}: P={p:.3f} R={r:.3f} F1={f1:.3f}\")\n",
    "    lines.insert(0, f\"Macro-F1={macro / max(1, len(label_set)):.4f}\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "class Adam:\n",
    "    def __init__(\n",
    "        self, params, grads, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0.0\n",
    "    ):\n",
    "        self.params, self.grads = params, grads\n",
    "        self.lr, (self.b1, self.b2), self.eps = lr, betas, eps\n",
    "        self.weight_decay = weight_decay\n",
    "        self.m = [np.zeros_like(p) for p in params]\n",
    "        self.v = [np.zeros_like(p) for p in params]\n",
    "        self.t = 0\n",
    "\n",
    "    def step(self):\n",
    "        self.t += 1\n",
    "        b1t = 1 - self.b1**self.t\n",
    "        b2t = 1 - self.b2**self.t\n",
    "        for i, (p, g) in enumerate(zip(self.params, self.grads)):\n",
    "            g = g + self.weight_decay * p if self.weight_decay != 0.0 else g\n",
    "            self.m[i] = self.b1 * self.m[i] + (1 - self.b1) * g\n",
    "            self.v[i] = self.b2 * self.v[i] + (1 - self.b2) * (g * g)\n",
    "            mhat = self.m[i] / (b1t + 1e-12)\n",
    "            vhat = self.v[i] / (b2t + 1e-12)\n",
    "            p -= self.lr * mhat / (np.sqrt(vhat) + self.eps)\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for g in self.grads:\n",
    "            g.fill(0.0)\n",
    "\n",
    "\n",
    "def batch_iter(X, M, y, batch_size, shuffle=True):\n",
    "    N = X.shape[0]\n",
    "    idx = np.arange(N)\n",
    "    if shuffle:\n",
    "        np.random.shuffle(idx)\n",
    "    for i in range(0, N, batch_size):\n",
    "        j = idx[i : i + batch_size]\n",
    "        yield X[j], M[j], y[j]\n",
    "\n",
    "\n",
    "def global_grad_clip(grads, max_norm=1.0):\n",
    "    total = 0.0\n",
    "    for g in grads:\n",
    "        total += float(np.sum(g * g))\n",
    "    global_norm = math.sqrt(total)\n",
    "    if global_norm > max_norm:\n",
    "        factor = max_norm / (global_norm + 1e-12)\n",
    "        for g in grads:\n",
    "            g *= factor\n",
    "\n",
    "\n",
    "class Dropout:\n",
    "    def __init__(self, p=0.0):\n",
    "        self.p = p\n",
    "        self.training = True\n",
    "        self.last_mask = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        if not self.training or self.p == 0.0:\n",
    "            return x\n",
    "        keep_prob = 1.0 - self.p\n",
    "        mask = np.random.binomial(1, keep_prob, x.shape).astype(np.float32)\n",
    "        self.last_mask = mask\n",
    "        return x * mask / keep_prob\n",
    "\n",
    "    def backward(self, dy):\n",
    "        if not self.training or self.p == 0.0:\n",
    "            return dy\n",
    "        keep_prob = 1.0 - self.p\n",
    "        return dy * self.last_mask / keep_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer components: positional encoding and layer normalization\n",
    "def create_positional_encoding(max_len, emb_dim):\n",
    "    pe = np.zeros((max_len, emb_dim), dtype=np.float32)\n",
    "    position = np.arange(max_len, dtype=np.float32).reshape(-1, 1)\n",
    "\n",
    "    div_term = np.exp(\n",
    "        np.arange(0, emb_dim, 2, dtype=np.float32) * -(math.log(10000.0) / emb_dim)\n",
    "    )\n",
    "\n",
    "    pe[:, 0::2] = np.sin(position * div_term)\n",
    "    pe[:, 1::2] = np.cos(position * div_term)\n",
    "\n",
    "    return pe\n",
    "\n",
    "\n",
    "class LayerNorm:\n",
    "    def __init__(self, dim, eps=1e-5):\n",
    "        self.dim = dim\n",
    "        self.eps = eps\n",
    "\n",
    "        self.gamma = np.ones(dim, dtype=np.float32)\n",
    "        self.beta = np.zeros(dim, dtype=np.float32)\n",
    "\n",
    "        self.dgamma = np.zeros_like(self.gamma)\n",
    "        self.dbeta = np.zeros_like(self.beta)\n",
    "\n",
    "        self.cache = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = np.mean(x, axis=-1, keepdims=True)\n",
    "        var = np.var(x, axis=-1, keepdims=True)\n",
    "        x_norm = (x - mean) / np.sqrt(var + self.eps)\n",
    "\n",
    "        out = self.gamma * x_norm + self.beta\n",
    "\n",
    "        self.cache = (x, mean, var, x_norm)\n",
    "        return out\n",
    "\n",
    "    def backward(self, dy):\n",
    "        x, mean, var, x_norm = self.cache\n",
    "\n",
    "        axis = tuple(range(dy.ndim - 1))\n",
    "        self.dbeta[...] = np.sum(dy, axis=axis)\n",
    "        self.dgamma[...] = np.sum(dy * x_norm, axis=axis)\n",
    "\n",
    "        dx_norm = dy * self.gamma\n",
    "        std_inv = 1.0 / np.sqrt(var + self.eps)\n",
    "\n",
    "        dvar = np.sum(\n",
    "            dx_norm * (x - mean) * -0.5 * (std_inv**3), axis=-1, keepdims=True\n",
    "        )\n",
    "        dmean = np.sum(-dx_norm * std_inv, axis=-1, keepdims=True) + dvar * np.mean(\n",
    "            -2.0 * (x - mean), axis=-1, keepdims=True\n",
    "        )\n",
    "\n",
    "        dx = (\n",
    "            (dx_norm * std_inv)\n",
    "            + (dvar * 2.0 * (x - mean) / x.shape[-1])\n",
    "            + (dmean / x.shape[-1])\n",
    "        )\n",
    "\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-head attention mechanism for Transformer\n",
    "def _softmax(x, axis=-1):\n",
    "    e_x = np.exp(x - np.max(x, axis=axis, keepdims=True))\n",
    "    return e_x / np.sum(e_x, axis=axis, keepdims=True)\n",
    "\n",
    "\n",
    "def _dsoftmax(dy, y):\n",
    "    y_reshaped = y[..., np.newaxis]\n",
    "    dy_reshaped = dy[..., np.newaxis]\n",
    "\n",
    "    diag_J = np.einsum(\"...ij,...ik->...ijk\", y, np.eye(y.shape[-1]))\n",
    "    outer_prod = np.einsum(\"...i,...j->...ij\", y, y)\n",
    "    J = diag_J - outer_prod\n",
    "\n",
    "    d_out = y * dy - y * np.sum(y * dy, axis=-1, keepdims=True)\n",
    "    return d_out\n",
    "\n",
    "\n",
    "class MultiheadAttention:\n",
    "    def __init__(self, emb_dim, num_heads):\n",
    "        assert emb_dim % num_heads == 0, (\n",
    "            \"Embedding dimension must be divisible by num_heads\"\n",
    "        )\n",
    "        self.emb_dim = emb_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = emb_dim // num_heads\n",
    "\n",
    "        self.in_proj = Linear(emb_dim, 3 * emb_dim)\n",
    "        self.out_proj = Linear(emb_dim, emb_dim)\n",
    "\n",
    "        self.cache = None\n",
    "\n",
    "    def forward(self, x, key_padding_mask):\n",
    "        B, T, D = x.shape\n",
    "\n",
    "        qkv = self.in_proj.forward(x)\n",
    "        q, k, v = np.split(qkv, 3, axis=-1)\n",
    "\n",
    "        q = q.reshape(B, T, self.num_heads, self.head_dim).transpose(0, 2, 1, 3)\n",
    "        k = k.reshape(B, T, self.num_heads, self.head_dim).transpose(0, 2, 1, 3)\n",
    "        v = v.reshape(B, T, self.num_heads, self.head_dim).transpose(0, 2, 1, 3)\n",
    "\n",
    "        scores = (q @ k.transpose(0, 1, 3, 2)) / np.sqrt(self.head_dim)\n",
    "\n",
    "        mask_reshaped = key_padding_mask[:, np.newaxis, np.newaxis, :]\n",
    "        scores = np.where(mask_reshaped == 0, -1e9, scores)\n",
    "\n",
    "        attn_weights = _softmax(scores, axis=-1)\n",
    "\n",
    "        context = attn_weights @ v\n",
    "\n",
    "        context_concatenated = context.transpose(0, 2, 1, 3).reshape(B, T, self.emb_dim)\n",
    "        output = self.out_proj.forward(context_concatenated)\n",
    "\n",
    "        self.cache = (x, q, k, v, attn_weights, context_concatenated)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def backward(self, dy):\n",
    "        x, q, k, v, attn_weights, context_concatenated = self.cache\n",
    "        B, T, D = dy.shape\n",
    "\n",
    "        d_context_concatenated = self.out_proj.backward(dy)\n",
    "\n",
    "        d_context = d_context_concatenated.reshape(\n",
    "            B, T, self.num_heads, self.head_dim\n",
    "        ).transpose(0, 2, 1, 3)\n",
    "\n",
    "        d_attn_weights = d_context @ v.transpose(0, 1, 3, 2)\n",
    "        dv = attn_weights.transpose(0, 1, 3, 2) @ d_context\n",
    "\n",
    "        d_scores = _dsoftmax(d_attn_weights, attn_weights)\n",
    "\n",
    "        d_scores /= np.sqrt(self.head_dim)\n",
    "\n",
    "        dq = d_scores @ k\n",
    "        dk = d_scores.transpose(0, 1, 3, 2) @ q\n",
    "\n",
    "        dq = dq.transpose(0, 2, 1, 3).reshape(B, T, self.emb_dim)\n",
    "        dk = dk.transpose(0, 2, 1, 3).reshape(B, T, self.emb_dim)\n",
    "        dv = dv.transpose(0, 2, 1, 3).reshape(B, T, self.emb_dim)\n",
    "\n",
    "        d_qkv = np.concatenate([dq, dk, dv], axis=-1)\n",
    "        dx = self.in_proj.backward(d_qkv)\n",
    "\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer encoder layer with self-attention and feed-forward network\n",
    "class TransformerEncoderLayer:\n",
    "    def __init__(self, emb_dim, num_heads, dim_feedforward, dropout_p=0.1):\n",
    "        self.self_attn = MultiheadAttention(emb_dim, num_heads)\n",
    "        self.norm1 = LayerNorm(emb_dim)\n",
    "        self.dropout1 = Dropout(dropout_p)\n",
    "\n",
    "        self.linear1 = Linear(emb_dim, dim_feedforward)\n",
    "        self.linear2 = Linear(dim_feedforward, emb_dim)\n",
    "        self.norm2 = LayerNorm(emb_dim)\n",
    "        self.dropout2 = Dropout(dropout_p)\n",
    "\n",
    "        self.cache = {}\n",
    "\n",
    "    def train(self):\n",
    "        self.dropout1.training = True\n",
    "        self.dropout2.training = True\n",
    "\n",
    "    def eval(self):\n",
    "        self.dropout1.training = False\n",
    "        self.dropout2.training = False\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        attn_output = self.self_attn.forward(x, mask)\n",
    "        attn_output_dropped = self.dropout1.forward(attn_output)\n",
    "        sublayer1_out = x + attn_output_dropped\n",
    "        norm1_out = self.norm1.forward(sublayer1_out)\n",
    "\n",
    "        linear1_out = self.linear1.forward(norm1_out)\n",
    "        relu_out = np.maximum(0, linear1_out)\n",
    "        linear2_out = self.linear2.forward(relu_out)\n",
    "        ffn_output_dropped = self.dropout2.forward(linear2_out)\n",
    "        sublayer2_out = norm1_out + ffn_output_dropped\n",
    "        norm2_out = self.norm2.forward(sublayer2_out)\n",
    "\n",
    "        self.cache[\"x\"] = x\n",
    "        self.cache[\"norm1_out\"] = norm1_out\n",
    "        self.cache[\"relu_out\"] = relu_out\n",
    "\n",
    "        return norm2_out\n",
    "\n",
    "    def backward(self, dy):\n",
    "        x = self.cache[\"x\"]\n",
    "        norm1_out = self.cache[\"norm1_out\"]\n",
    "        relu_out = self.cache[\"relu_out\"]\n",
    "\n",
    "        d_sublayer2_out = self.norm2.backward(dy)\n",
    "        d_norm1_out_from_res = d_sublayer2_out\n",
    "        d_ffn_output_dropped = d_sublayer2_out\n",
    "\n",
    "        d_linear2_out = self.dropout2.backward(d_ffn_output_dropped)\n",
    "        d_relu_out = self.linear2.backward(d_linear2_out)\n",
    "        d_linear1_out = d_relu_out * (relu_out > 0)\n",
    "        d_norm1_out_from_ffn = self.linear1.backward(d_linear1_out)\n",
    "\n",
    "        d_norm1_out = d_norm1_out_from_res + d_norm1_out_from_ffn\n",
    "\n",
    "        d_sublayer1_out = self.norm1.backward(d_norm1_out)\n",
    "        dx_from_res = d_sublayer1_out\n",
    "        d_attn_output_dropped = d_sublayer1_out\n",
    "\n",
    "        d_attn_output = self.dropout1.backward(d_attn_output_dropped)\n",
    "        dx_from_attn = self.self_attn.backward(d_attn_output)\n",
    "\n",
    "        dx = dx_from_res + dx_from_attn\n",
    "\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop with early stopping and validation monitoring\n",
    "def train_model(\n",
    "    model,\n",
    "    train_data,\n",
    "    val_data,\n",
    "    num_classes,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    lr=1e-3,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    early_patience=EARLY_STOP,\n",
    "    max_grad_norm=1.0,\n",
    "    id2label=None,\n",
    "):\n",
    "    Xtr, Mtr, ytr = train_data\n",
    "    Xva, Mva, yva = val_data\n",
    "\n",
    "    class_w = compute_class_weights(ytr, num_classes)\n",
    "    print(\"Class counts:\", np.bincount(ytr, minlength=num_classes))\n",
    "    print(\"Class weights:\", class_w)\n",
    "\n",
    "    params, grads = model.parameters_and_grads()\n",
    "    opt = Adam(params, grads, lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    best_f1, best_state, patience = -1.0, None, early_patience\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        opt.zero_grad()\n",
    "        losses = []\n",
    "        t0 = time.time()\n",
    "        num_batches = int(np.ceil(len(Xtr) / batch_size))\n",
    "        last_gnorm = None\n",
    "\n",
    "        for b, (xb, mb, yb) in enumerate(\n",
    "            batch_iter(Xtr, Mtr, ytr, batch_size, shuffle=True), 1\n",
    "        ):\n",
    "            logits = model.forward(xb, mb)\n",
    "            loss, dlog = weighted_cross_entropy(logits, yb, class_w)\n",
    "            model.backward(dlog)\n",
    "\n",
    "            gsum = 0.0\n",
    "            for g in grads:\n",
    "                gsum += float(np.sum(g * g))\n",
    "            last_gnorm = math.sqrt(gsum)\n",
    "\n",
    "            global_grad_clip(grads, max_grad_norm)\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "            losses.append(loss)\n",
    "\n",
    "            elapsed = time.time() - t0\n",
    "            print(\n",
    "                f\"\\r  batch {b:>4}/{num_batches} | avg_loss={np.mean(losses):.4f} | grad_norm={last_gnorm:.3e} | {elapsed:.1f}s\",\n",
    "                end=\"\",\n",
    "                flush=True,\n",
    "            )\n",
    "\n",
    "        dur = time.time() - t0\n",
    "        print(f\"\\nend-epoch grad_norm={last_gnorm:.3e}\")\n",
    "\n",
    "        model.eval()\n",
    "        logits = []\n",
    "        for xb, mb, yb in batch_iter(Xva, Mva, yva, batch_size=256, shuffle=False):\n",
    "            logits.append(model.forward(xb, mb))\n",
    "        logits = np.concatenate(logits, axis=0)\n",
    "        y_pred = np.argmax(logits, axis=1)\n",
    "\n",
    "        labels = sorted(set(yva.tolist()) | set(y_pred.tolist()))\n",
    "        f1_sum = 0.0\n",
    "        for c in labels:\n",
    "            tp = np.sum((yva == c) & (y_pred == c))\n",
    "            fp = np.sum((yva != c) & (y_pred == c))\n",
    "            fn = np.sum((yva == c) & (y_pred != c))\n",
    "            p = tp / (tp + fp + 1e-12)\n",
    "            r = tp / (tp + fn + 1e-12)\n",
    "            f1 = 2 * p * r / (p + r + 1e-12)\n",
    "            f1_sum += f1\n",
    "        macro_f1 = f1_sum / max(1, len(labels))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch:02d} | loss={np.mean(losses):.4f} | val_macro_f1={macro_f1:.4f} | {dur:.1f}s\"\n",
    "        )\n",
    "\n",
    "        if macro_f1 > best_f1 + 1e-6:\n",
    "            best_f1 = macro_f1\n",
    "            best_state = {\n",
    "                \"emb.W\": model.emb.W.copy(),\n",
    "                \"rnn.Wxh\": model.rnn.Wxh.copy(),\n",
    "                \"rnn.Whh\": model.rnn.Whh.copy(),\n",
    "                \"rnn.bh\": model.rnn.bh.copy(),\n",
    "                \"fc.W\": model.fc.W.copy(),\n",
    "                \"fc.b\": model.fc.b.copy(),\n",
    "            }\n",
    "            patience = early_patience\n",
    "        else:\n",
    "            patience -= 1\n",
    "            if patience <= 0:\n",
    "                print(\"Early stopping.\")\n",
    "                break\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.emb.W[:] = best_state[\"emb.W\"]\n",
    "        model.rnn.Wxh[:] = best_state[\"rnn.Wxh\"]\n",
    "        model.rnn.Whh[:] = best_state[\"rnn.Whh\"]\n",
    "        model.rnn.bh[:] = best_state[\"rnn.bh\"]\n",
    "        model.fc.W[:] = best_state[\"fc.W\"]\n",
    "        model.fc.b[:] = best_state[\"fc.b\"]\n",
    "\n",
    "    model.eval()\n",
    "    logits = []\n",
    "    for xb, mb, yb in batch_iter(Xva, Mva, yva, batch_size=256, shuffle=False):\n",
    "        logits.append(model.forward(xb, mb))\n",
    "    logits = np.concatenate(logits, axis=0)\n",
    "    y_pred = np.argmax(logits, axis=1)\n",
    "    print(\"\\nValidation report:\\n\" + classification_report(yva, y_pred, id2label))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN model save and load functions for artifacts and weights\n",
    "def save_rnn_artifacts(out_dir, tokenizer, label2id, config, model):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    tok_path = os.path.join(out_dir, \"tokenizer.json\")\n",
    "    with open(tok_path, \"w\") as f:\n",
    "        json.dump(\n",
    "            {\n",
    "                \"token2id\": tokenizer.token2id,\n",
    "                \"id2token\": tokenizer.id2token,\n",
    "                \"max_vocab\": tokenizer.max_vocab,\n",
    "                \"min_freq\": tokenizer.min_freq,\n",
    "            },\n",
    "            f,\n",
    "        )\n",
    "\n",
    "    label_path = os.path.join(out_dir, \"labels.json\")\n",
    "    with open(label_path, \"w\") as f:\n",
    "        json.dump(label2id, f)\n",
    "\n",
    "    config_path = os.path.join(out_dir, \"config.json\")\n",
    "    with open(config_path, \"w\") as f:\n",
    "        json.dump(config, f)\n",
    "\n",
    "    weights_path = os.path.join(out_dir, \"weights_rnn_best.npz\")\n",
    "    np.savez(\n",
    "        weights_path,\n",
    "        emb_W=model.emb.W,\n",
    "        rnn_Wxh=model.rnn.Wxh,\n",
    "        rnn_Whh=model.rnn.Whh,\n",
    "        rnn_bh=model.rnn.bh,\n",
    "        fc_W=model.fc.W,\n",
    "        fc_b=model.fc.b,\n",
    "    )\n",
    "    print(f\"âœ… Saved artifacts to {out_dir}/\")\n",
    "\n",
    "\n",
    "def load_rnn_weights(weights_path, model):\n",
    "    data = np.load(weights_path)\n",
    "    model.emb.W[:] = data[\"emb_W\"]\n",
    "    model.rnn.Wxh[:] = data[\"rnn_Wxh\"]\n",
    "    model.rnn.Whh[:] = data[\"rnn_Whh\"]\n",
    "    model.rnn.bh[:] = data[\"rnn_bh\"]\n",
    "    model.fc.W[:] = data[\"fc_W\"]\n",
    "    model.fc.b[:] = data[\"fc_b\"]\n",
    "    print(f\"âœ… Loaded weights from {weights_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM model save and load functions for artifacts and weights\n",
    "def save_lstm_artifacts(out_dir, tokenizer, label2id, config, model):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    with open(os.path.join(out_dir, \"vocab.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump({\"token2id\": tokenizer.token2id, \"id2token\": tokenizer.id2token}, f)\n",
    "    with open(os.path.join(out_dir, \"label2id.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(label2id, f)\n",
    "    with open(os.path.join(out_dir, \"config.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(config, f)\n",
    "    np.savez(\n",
    "        os.path.join(out_dir, \"weights_lstm_best.npz\"),\n",
    "        emb_W=model.emb.W,\n",
    "        lstm_W_ih=model.lstm.W_ih,\n",
    "        lstm_W_hh=model.lstm.W_hh,\n",
    "        lstm_b=model.lstm.b,\n",
    "        fc_W=model.fc.W,\n",
    "        fc_b=model.fc.b,\n",
    "    )\n",
    "\n",
    "\n",
    "def load_lstm_weights(npz_path, model):\n",
    "    z = np.load(npz_path)\n",
    "    model.emb.W[:] = z[\"emb_W\"]\n",
    "    model.lstm.W_ih[:] = z[\"lstm_W_ih\"]\n",
    "    model.lstm.W_hh[:] = z[\"lstm_W_hh\"]\n",
    "    model.lstm.b[:] = z[\"lstm_b\"]\n",
    "    model.fc.W[:] = z[\"fc_W\"]\n",
    "    model.fc.b[:] = z[\"fc_b\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer model save and load functions for artifacts and weights\n",
    "def save_transformer_artifacts(out_dir, tokenizer, label2id, config, model):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    with open(os.path.join(out_dir, \"vocab.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump({\"token2id\": tokenizer.token2id, \"id2token\": tokenizer.id2token}, f)\n",
    "    with open(os.path.join(out_dir, \"label2id.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(label2id, f)\n",
    "    with open(os.path.join(out_dir, \"config.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(config, f)\n",
    "\n",
    "    weights_to_save = {\n",
    "        \"emb_W\": model.emb.W,\n",
    "        \"fc_W\": model.fc.W,\n",
    "        \"fc_b\": model.fc.b,\n",
    "    }\n",
    "\n",
    "    for i, layer in enumerate(model.encoder_layers):\n",
    "        weights_to_save[f\"layer_{i}_attn_in_proj_W\"] = layer.self_attn.in_proj.W\n",
    "        weights_to_save[f\"layer_{i}_attn_in_proj_b\"] = layer.self_attn.in_proj.b\n",
    "        weights_to_save[f\"layer_{i}_attn_out_proj_W\"] = layer.self_attn.out_proj.W\n",
    "        weights_to_save[f\"layer_{i}_attn_out_proj_b\"] = layer.self_attn.out_proj.b\n",
    "        weights_to_save[f\"layer_{i}_linear1_W\"] = layer.linear1.W\n",
    "        weights_to_save[f\"layer_{i}_linear1_b\"] = layer.linear1.b\n",
    "        weights_to_save[f\"layer_{i}_linear2_W\"] = layer.linear2.W\n",
    "        weights_to_save[f\"layer_{i}_linear2_b\"] = layer.linear2.b\n",
    "        weights_to_save[f\"layer_{i}_norm1_gamma\"] = layer.norm1.gamma\n",
    "        weights_to_save[f\"layer_{i}_norm1_beta\"] = layer.norm1.beta\n",
    "        weights_to_save[f\"layer_{i}_norm2_gamma\"] = layer.norm2.gamma\n",
    "        weights_to_save[f\"layer_{i}_norm2_beta\"] = layer.norm2.beta\n",
    "\n",
    "    np.savez(os.path.join(out_dir, \"weights_transformer_best.npz\"), **weights_to_save)\n",
    "    print(f\"âœ… Saved Transformer artifacts to {out_dir}/\")\n",
    "\n",
    "\n",
    "def load_transformer_weights(npz_path, model):\n",
    "    z = np.load(npz_path)\n",
    "    model.emb.W[:] = z[\"emb_W\"]\n",
    "    model.fc.W[:] = z[\"fc_W\"]\n",
    "    model.fc.b[:] = z[\"fc_b\"]\n",
    "\n",
    "    for i, layer in enumerate(model.encoder_layers):\n",
    "        layer.self_attn.in_proj.W[:] = z[f\"layer_{i}_attn_in_proj_W\"]\n",
    "        layer.self_attn.in_proj.b[:] = z[f\"layer_{i}_attn_in_proj_b\"]\n",
    "        layer.self_attn.out_proj.W[:] = z[f\"layer_{i}_attn_out_proj_W\"]\n",
    "        layer.self_attn.out_proj.b[:] = z[f\"layer_{i}_attn_out_proj_b\"]\n",
    "        layer.linear1.W[:] = z[f\"layer_{i}_linear1_W\"]\n",
    "        layer.linear1.b[:] = z[f\"layer_{i}_linear1_b\"]\n",
    "        layer.linear2.W[:] = z[f\"layer_{i}_linear2_W\"]\n",
    "        layer.linear2.b[:] = z[f\"layer_{i}_linear2_b\"]\n",
    "        layer.norm1.gamma[:] = z[f\"layer_{i}_norm1_gamma\"]\n",
    "        layer.norm1.beta[:] = z[f\"layer_{i}_norm1_beta\"]\n",
    "        layer.norm2.gamma[:] = z[f\"layer_{i}_norm2_gamma\"]\n",
    "        layer.norm2.beta[:] = z[f\"layer_{i}_norm2_beta\"]\n",
    "    print(f\"âœ… Loaded Transformer weights from {npz_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "eqNoZg3Y9eJ2"
   },
   "outputs": [],
   "source": [
    "# RNN-based text classifier with embedding, RNN, and fully connected layers\n",
    "class RNNClassifier:\n",
    "    def __init__(\n",
    "        self, vocab_size, num_classes, emb_dim=100, hidden_dim=256, dropout=0.0\n",
    "    ):\n",
    "        self.emb = Embedding(vocab_size, emb_dim)\n",
    "        self.rnn = RNN(emb_dim, hidden_dim)\n",
    "        self.drop_emb = Dropout(dropout)\n",
    "        self.drop_pool = Dropout(dropout)\n",
    "        self.fc = Linear(hidden_dim, num_classes)\n",
    "        self.training = True\n",
    "        self._cache = None\n",
    "\n",
    "    def parameters_and_grads(self):\n",
    "        params = [\n",
    "            self.emb.W,\n",
    "            self.rnn.Wxh,\n",
    "            self.rnn.Whh,\n",
    "            self.rnn.bh,\n",
    "            self.fc.W,\n",
    "            self.fc.b,\n",
    "        ]\n",
    "        grads = [\n",
    "            self.emb.grad,\n",
    "            self.rnn.dWxh,\n",
    "            self.rnn.dWhh,\n",
    "            self.rnn.dbh,\n",
    "            self.fc.dW,\n",
    "            self.fc.db,\n",
    "        ]\n",
    "        return params, grads\n",
    "\n",
    "    def train(self):\n",
    "        self.training = True\n",
    "        self.drop_emb.training = True\n",
    "        self.drop_pool.training = True\n",
    "\n",
    "    def eval(self):\n",
    "        self.training = False\n",
    "        self.drop_emb.training = False\n",
    "        self.drop_pool.training = False\n",
    "\n",
    "    def forward(self, x_ids, mask):\n",
    "        E = self.emb.forward(x_ids)\n",
    "        E = self.drop_emb.forward(E)\n",
    "        H = self.rnn.forward(E, mask)\n",
    "        lengths = np.sum(mask, axis=1).astype(np.int32)\n",
    "        last_idx = np.clip(lengths - 1, 0, mask.shape[1] - 1)\n",
    "        pooled = H[np.arange(H.shape[0]), last_idx]\n",
    "        pooled = self.drop_pool.forward(pooled)\n",
    "        logits = self.fc.forward(pooled)\n",
    "        self._cache = (lengths,)\n",
    "        return logits\n",
    "\n",
    "    def backward(self, dlogits):\n",
    "        dpooled = self.fc.backward(dlogits)\n",
    "        dpooled = self.drop_pool.backward(dpooled)\n",
    "\n",
    "        (lengths,) = self._cache\n",
    "        B, T, H = self.rnn.last_h.shape\n",
    "        dH = np.zeros_like(self.rnn.last_h)\n",
    "        last_idx = np.clip(lengths - 1, 0, T - 1)\n",
    "        dH[np.arange(B), last_idx] = dpooled\n",
    "\n",
    "        dE = self.rnn.backward(dH)\n",
    "        dE = self.drop_emb.backward(dE)\n",
    "        self.emb.backward(dE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM-based text classifier with embedding, LSTM, and fully connected layers\n",
    "class LSTMClassifier:\n",
    "    def __init__(\n",
    "        self, vocab_size, num_classes, emb_dim=100, hidden_dim=256, dropout=0.0\n",
    "    ):\n",
    "        self.emb = Embedding(vocab_size, emb_dim)\n",
    "        self.lstm = LSTM(emb_dim, hidden_dim)\n",
    "        self.drop_emb = Dropout(dropout)\n",
    "        self.drop_pool = Dropout(dropout)\n",
    "        self.fc = Linear(hidden_dim, num_classes)\n",
    "        self.training = True\n",
    "        self._cache = None\n",
    "\n",
    "    def parameters_and_grads(self):\n",
    "        params = [\n",
    "            self.emb.W,\n",
    "            self.lstm.W_ih,\n",
    "            self.lstm.W_hh,\n",
    "            self.lstm.b,\n",
    "            self.fc.W,\n",
    "            self.fc.b,\n",
    "        ]\n",
    "        grads = [\n",
    "            self.emb.grad,\n",
    "            self.lstm.dW_ih,\n",
    "            self.lstm.dW_hh,\n",
    "            self.lstm.db,\n",
    "            self.fc.dW,\n",
    "            self.fc.db,\n",
    "        ]\n",
    "        return params, grads\n",
    "\n",
    "    def train(self):\n",
    "        self.training = True\n",
    "        self.drop_emb.training = True\n",
    "        self.drop_pool.training = True\n",
    "\n",
    "    def eval(self):\n",
    "        self.training = False\n",
    "        self.drop_emb.training = False\n",
    "        self.drop_pool.training = False\n",
    "\n",
    "    def forward(self, x_ids, mask):\n",
    "        E = self.emb.forward(x_ids)\n",
    "        E = self.drop_emb.forward(E)\n",
    "        H = self.lstm.forward(E, mask)\n",
    "        lengths = np.sum(mask, axis=1).astype(np.int32)\n",
    "        last_idx = np.clip(lengths - 1, 0, mask.shape[1] - 1)\n",
    "        pooled = H[np.arange(H.shape[0]), last_idx]\n",
    "        pooled = self.drop_pool.forward(pooled)\n",
    "        logits = self.fc.forward(pooled)\n",
    "        self._cache = (lengths, H)\n",
    "        return logits\n",
    "\n",
    "    def backward(self, dlogits):\n",
    "        dpooled = self.fc.backward(dlogits)\n",
    "        dpooled = self.drop_pool.backward(dpooled)\n",
    "\n",
    "        lengths, H = self._cache\n",
    "        B, T, hidden_dim = H.shape\n",
    "        dH = np.zeros_like(H)\n",
    "        last_idx = np.clip(lengths - 1, 0, T - 1)\n",
    "        dH[np.arange(B), last_idx] = dpooled\n",
    "\n",
    "        dE = self.lstm.backward(dH)\n",
    "        dE = self.drop_emb.backward(dE)\n",
    "        self.emb.backward(dE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM-specific model training function adapted for LSTM attributes\n",
    "def train_lstm_model(\n",
    "    model,\n",
    "    train_data,\n",
    "    val_data,\n",
    "    num_classes,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    lr=1e-3,\n",
    "    weight_decay=1e-4,\n",
    "    early_patience=5,\n",
    "    max_grad_norm=1.0,\n",
    "    id2label=None,\n",
    "):\n",
    "    Xtr, Mtr, ytr = train_data\n",
    "    Xva, Mva, yva = val_data\n",
    "\n",
    "    class_w = compute_class_weights(ytr, num_classes)\n",
    "    print(\"Class counts:\", np.bincount(ytr, minlength=num_classes))\n",
    "    print(\"Class weights:\", class_w)\n",
    "\n",
    "    params, grads = model.parameters_and_grads()\n",
    "    opt = Adam(params, grads, lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    best_f1, best_state, patience = -1.0, None, early_patience\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        opt.zero_grad()\n",
    "        losses = []\n",
    "        t0 = time.time()\n",
    "        num_batches = int(np.ceil(len(Xtr) / batch_size))\n",
    "        last_gnorm = None\n",
    "\n",
    "        for b, (xb, mb, yb) in enumerate(\n",
    "            batch_iter(Xtr, Mtr, ytr, batch_size, shuffle=True), 1\n",
    "        ):\n",
    "            logits = model.forward(xb, mb)\n",
    "            loss, dlog = weighted_cross_entropy(logits, yb, class_w)\n",
    "            model.backward(dlog)\n",
    "\n",
    "            gsum = 0.0\n",
    "            for g in grads:\n",
    "                gsum += float(np.sum(g * g))\n",
    "            last_gnorm = math.sqrt(gsum)\n",
    "\n",
    "            global_grad_clip(grads, max_grad_norm)\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "            losses.append(loss)\n",
    "\n",
    "            elapsed = time.time() - t0\n",
    "            print(\n",
    "                f\"\\r  batch {b:>4}/{num_batches} | avg_loss={np.mean(losses):.4f} | grad_norm={last_gnorm:.3e} | {elapsed:.1f}s\",\n",
    "                end=\"\",\n",
    "                flush=True,\n",
    "            )\n",
    "\n",
    "        dur = time.time() - t0\n",
    "        print(f\"\\nend-epoch grad_norm={last_gnorm:.3e}\")\n",
    "\n",
    "        model.eval()\n",
    "        logits = []\n",
    "        for xb, mb, yb in batch_iter(Xva, Mva, yva, batch_size=256, shuffle=False):\n",
    "            logits.append(model.forward(xb, mb))\n",
    "        logits = np.concatenate(logits, axis=0)\n",
    "        y_pred = np.argmax(logits, axis=1)\n",
    "\n",
    "        labels = sorted(set(yva.tolist()) | set(y_pred.tolist()))\n",
    "        f1_sum = 0.0\n",
    "        for c in labels:\n",
    "            tp = np.sum((yva == c) & (y_pred == c))\n",
    "            fp = np.sum((yva != c) & (y_pred == c))\n",
    "            fn = np.sum((yva == c) & (y_pred != c))\n",
    "            p = tp / (tp + fp + 1e-12)\n",
    "            r = tp / (tp + fn + 1e-12)\n",
    "            f1 = 2 * p * r / (p + r + 1e-12)\n",
    "            f1_sum += f1\n",
    "        macro_f1 = f1_sum / max(1, len(labels))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch:02d} | loss={np.mean(losses):.4f} | val_macro_f1={macro_f1:.4f} | {dur:.1f}s\"\n",
    "        )\n",
    "\n",
    "        if macro_f1 > best_f1 + 1e-6:\n",
    "            best_f1 = macro_f1\n",
    "            best_state = {\n",
    "                \"emb.W\": model.emb.W.copy(),\n",
    "                \"lstm.W_ih\": model.lstm.W_ih.copy(),\n",
    "                \"lstm.W_hh\": model.lstm.W_hh.copy(),\n",
    "                \"lstm.b\": model.lstm.b.copy(),\n",
    "                \"fc.W\": model.fc.W.copy(),\n",
    "                \"fc.b\": model.fc.b.copy(),\n",
    "            }\n",
    "            patience = early_patience\n",
    "        else:\n",
    "            patience -= 1\n",
    "            if patience <= 0:\n",
    "                print(\"Early stopping.\")\n",
    "                break\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.emb.W[:] = best_state[\"emb.W\"]\n",
    "        model.lstm.W_ih[:] = best_state[\"lstm.W_ih\"]\n",
    "        model.lstm.W_hh[:] = best_state[\"lstm.W_hh\"]\n",
    "        model.lstm.b[:] = best_state[\"lstm.b\"]\n",
    "        model.fc.W[:] = best_state[\"fc.W\"]\n",
    "        model.fc.b[:] = best_state[\"fc.b\"]\n",
    "\n",
    "    model.eval()\n",
    "    logits = []\n",
    "    for xb, mb, yb in batch_iter(Xva, Mva, yva, batch_size=256, shuffle=False):\n",
    "        logits.append(model.forward(xb, mb))\n",
    "    logits = np.concatenate(logits, axis=0)\n",
    "    y_pred = np.argmax(logits, axis=1)\n",
    "    print(\"\\nValidation report:\\n\" + classification_report(yva, y_pred, id2label))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM model save and load functions for artifacts and weights\n",
    "def save_lstm_artifacts(out_dir, tokenizer, label2id, config, model):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    tok_path = os.path.join(out_dir, \"tokenizer.json\")\n",
    "    with open(tok_path, \"w\") as f:\n",
    "        json.dump(\n",
    "            {\n",
    "                \"token2id\": tokenizer.token2id,\n",
    "                \"id2token\": tokenizer.id2token,\n",
    "                \"max_vocab\": tokenizer.max_vocab,\n",
    "                \"min_freq\": tokenizer.min_freq,\n",
    "            },\n",
    "            f,\n",
    "        )\n",
    "\n",
    "    label_path = os.path.join(out_dir, \"labels.json\")\n",
    "    with open(label_path, \"w\") as f:\n",
    "        json.dump(label2id, f)\n",
    "\n",
    "    config_path = os.path.join(out_dir, \"config.json\")\n",
    "    with open(config_path, \"w\") as f:\n",
    "        json.dump(config, f)\n",
    "\n",
    "    weights_path = os.path.join(out_dir, \"weights_lstm_best.npz\")\n",
    "    np.savez(\n",
    "        weights_path,\n",
    "        emb_W=model.emb.W,\n",
    "        lstm_W_ih=model.lstm.W_ih,\n",
    "        lstm_W_hh=model.lstm.W_hh,\n",
    "        lstm_b=model.lstm.b,\n",
    "        fc_W=model.fc.W,\n",
    "        fc_b=model.fc.b,\n",
    "    )\n",
    "    print(f\"âœ… Saved LSTM artifacts to {out_dir}/\")\n",
    "\n",
    "\n",
    "def load_lstm_weights(weights_path, model):\n",
    "    data = np.load(weights_path)\n",
    "    model.emb.W[:] = data[\"emb_W\"]\n",
    "    model.lstm.W_ih[:] = data[\"lstm_W_ih\"]\n",
    "    model.lstm.W_hh[:] = data[\"lstm_W_hh\"]\n",
    "    model.lstm.b[:] = data[\"lstm_b\"]\n",
    "    model.fc.W[:] = data[\"fc_W\"]\n",
    "    model.fc.b[:] = data[\"fc_b\"]\n",
    "    print(\"âœ… Loaded LSTM weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer-specific model training function adapted for Transformer attributes\n",
    "def train_transformer_model(\n",
    "    model,\n",
    "    train_data,\n",
    "    val_data,\n",
    "    num_classes,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    lr=1e-3,\n",
    "    weight_decay=1e-4,\n",
    "    early_patience=5,\n",
    "    max_grad_norm=1.0,\n",
    "    id2label=None,\n",
    "):\n",
    "    Xtr, Mtr, ytr = train_data\n",
    "    Xva, Mva, yva = val_data\n",
    "\n",
    "    class_w = compute_class_weights(ytr, num_classes)\n",
    "    print(\"Class counts:\", np.bincount(ytr, minlength=num_classes))\n",
    "    print(\"Class weights:\", class_w)\n",
    "\n",
    "    params, grads = model.parameters_and_grads()\n",
    "    opt = Adam(params, grads, lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    best_f1, best_state, patience = -1.0, None, early_patience\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        opt.zero_grad()\n",
    "        losses = []\n",
    "        t0 = time.time()\n",
    "        num_batches = int(np.ceil(len(Xtr) / batch_size))\n",
    "        last_gnorm = None\n",
    "\n",
    "        for b, (xb, mb, yb) in enumerate(\n",
    "            batch_iter(Xtr, Mtr, ytr, batch_size, shuffle=True), 1\n",
    "        ):\n",
    "            logits = model.forward(xb, mb)\n",
    "            loss, dlog = weighted_cross_entropy(logits, yb, class_w)\n",
    "            model.backward(dlog)\n",
    "\n",
    "            gsum = 0.0\n",
    "            for g in grads:\n",
    "                gsum += float(np.sum(g * g))\n",
    "            last_gnorm = math.sqrt(gsum)\n",
    "\n",
    "            global_grad_clip(grads, max_grad_norm)\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "            losses.append(loss)\n",
    "\n",
    "            elapsed = time.time() - t0\n",
    "            print(\n",
    "                f\"\\r  batch {b:>4}/{num_batches} | avg_loss={np.mean(losses):.4f} | grad_norm={last_gnorm:.3e} | {elapsed:.1f}s\",\n",
    "                end=\"\",\n",
    "                flush=True,\n",
    "            )\n",
    "\n",
    "        dur = time.time() - t0\n",
    "        print(f\"\\nend-epoch grad_norm={last_gnorm:.3e}\")\n",
    "\n",
    "        model.eval()\n",
    "        logits = []\n",
    "        for xb, mb, yb in batch_iter(Xva, Mva, yva, batch_size=256, shuffle=False):\n",
    "            logits.append(model.forward(xb, mb))\n",
    "        logits = np.concatenate(logits, axis=0)\n",
    "        y_pred = np.argmax(logits, axis=1)\n",
    "\n",
    "        labels = sorted(set(yva.tolist()) | set(y_pred.tolist()))\n",
    "        f1_sum = 0.0\n",
    "        for c in labels:\n",
    "            tp = np.sum((yva == c) & (y_pred == c))\n",
    "            fp = np.sum((yva != c) & (y_pred == c))\n",
    "            fn = np.sum((yva == c) & (y_pred != c))\n",
    "            p = tp / (tp + fp + 1e-12)\n",
    "            r = tp / (tp + fn + 1e-12)\n",
    "            f1 = 2 * p * r / (p + r + 1e-12)\n",
    "            f1_sum += f1\n",
    "        macro_f1 = f1_sum / max(1, len(labels))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch:02d} | loss={np.mean(losses):.4f} | val_macro_f1={macro_f1:.4f} | {dur:.1f}s\"\n",
    "        )\n",
    "\n",
    "        if macro_f1 > best_f1 + 1e-6:\n",
    "            best_f1 = macro_f1\n",
    "            # Save transformer state with all encoder layers\n",
    "            best_state = {\n",
    "                \"emb.W\": model.emb.W.copy(),\n",
    "                \"fc.W\": model.fc.W.copy(),\n",
    "                \"fc.b\": model.fc.b.copy(),\n",
    "            }\n",
    "            \n",
    "            # Save all encoder layer parameters\n",
    "            for i, layer in enumerate(model.encoder_layers):\n",
    "                best_state[f\"encoder_{i}.self_attn.in_proj.W\"] = layer.self_attn.in_proj.W.copy()\n",
    "                best_state[f\"encoder_{i}.self_attn.in_proj.b\"] = layer.self_attn.in_proj.b.copy()\n",
    "                best_state[f\"encoder_{i}.self_attn.out_proj.W\"] = layer.self_attn.out_proj.W.copy()\n",
    "                best_state[f\"encoder_{i}.self_attn.out_proj.b\"] = layer.self_attn.out_proj.b.copy()\n",
    "                best_state[f\"encoder_{i}.linear1.W\"] = layer.linear1.W.copy()\n",
    "                best_state[f\"encoder_{i}.linear1.b\"] = layer.linear1.b.copy()\n",
    "                best_state[f\"encoder_{i}.linear2.W\"] = layer.linear2.W.copy()\n",
    "                best_state[f\"encoder_{i}.linear2.b\"] = layer.linear2.b.copy()\n",
    "                best_state[f\"encoder_{i}.norm1.gamma\"] = layer.norm1.gamma.copy()\n",
    "                best_state[f\"encoder_{i}.norm1.beta\"] = layer.norm1.beta.copy()\n",
    "                best_state[f\"encoder_{i}.norm2.gamma\"] = layer.norm2.gamma.copy()\n",
    "                best_state[f\"encoder_{i}.norm2.beta\"] = layer.norm2.beta.copy()\n",
    "            \n",
    "            patience = early_patience\n",
    "        else:\n",
    "            patience -= 1\n",
    "            if patience <= 0:\n",
    "                print(\"Early stopping.\")\n",
    "                break\n",
    "\n",
    "    if best_state is not None:\n",
    "        # Restore best transformer state\n",
    "        model.emb.W[:] = best_state[\"emb.W\"]\n",
    "        model.fc.W[:] = best_state[\"fc.W\"]\n",
    "        model.fc.b[:] = best_state[\"fc.b\"]\n",
    "        \n",
    "        # Restore all encoder layer parameters\n",
    "        for i, layer in enumerate(model.encoder_layers):\n",
    "            layer.self_attn.in_proj.W[:] = best_state[f\"encoder_{i}.self_attn.in_proj.W\"]\n",
    "            layer.self_attn.in_proj.b[:] = best_state[f\"encoder_{i}.self_attn.in_proj.b\"]\n",
    "            layer.self_attn.out_proj.W[:] = best_state[f\"encoder_{i}.self_attn.out_proj.W\"]\n",
    "            layer.self_attn.out_proj.b[:] = best_state[f\"encoder_{i}.self_attn.out_proj.b\"]\n",
    "            layer.linear1.W[:] = best_state[f\"encoder_{i}.linear1.W\"]\n",
    "            layer.linear1.b[:] = best_state[f\"encoder_{i}.linear1.b\"]\n",
    "            layer.linear2.W[:] = best_state[f\"encoder_{i}.linear2.W\"]\n",
    "            layer.linear2.b[:] = best_state[f\"encoder_{i}.linear2.b\"]\n",
    "            layer.norm1.gamma[:] = best_state[f\"encoder_{i}.norm1.gamma\"]\n",
    "            layer.norm1.beta[:] = best_state[f\"encoder_{i}.norm1.beta\"]\n",
    "            layer.norm2.gamma[:] = best_state[f\"encoder_{i}.norm2.gamma\"]\n",
    "            layer.norm2.beta[:] = best_state[f\"encoder_{i}.norm2.beta\"]\n",
    "\n",
    "    model.eval()\n",
    "    logits = []\n",
    "    for xb, mb, yb in batch_iter(Xva, Mva, yva, batch_size=256, shuffle=False):\n",
    "        logits.append(model.forward(xb, mb))\n",
    "    logits = np.concatenate(logits, axis=0)\n",
    "    y_pred = np.argmax(logits, axis=1)\n",
    "    print(\"\\nValidation report:\\n\" + classification_report(yva, y_pred, id2label))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer model save and load functions for artifacts and weights\n",
    "def save_transformer_artifacts(out_dir, tokenizer, label2id, config, model):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    tok_path = os.path.join(out_dir, \"tokenizer.json\")\n",
    "    with open(tok_path, \"w\") as f:\n",
    "        json.dump(\n",
    "            {\n",
    "                \"token2id\": tokenizer.token2id,\n",
    "                \"id2token\": tokenizer.id2token,\n",
    "                \"max_vocab\": tokenizer.max_vocab,\n",
    "                \"min_freq\": tokenizer.min_freq,\n",
    "            },\n",
    "            f,\n",
    "        )\n",
    "\n",
    "    label_path = os.path.join(out_dir, \"labels.json\")\n",
    "    with open(label_path, \"w\") as f:\n",
    "        json.dump(label2id, f)\n",
    "\n",
    "    config_path = os.path.join(out_dir, \"config.json\")\n",
    "    with open(config_path, \"w\") as f:\n",
    "        json.dump(config, f)\n",
    "\n",
    "    weights_path = os.path.join(out_dir, \"weights_transformer_best.npz\")\n",
    "    \n",
    "    # Prepare weights dictionary for all Transformer parameters\n",
    "    weights_dict = {\n",
    "        \"emb_W\": model.emb.W,\n",
    "        \"fc_W\": model.fc.W,\n",
    "        \"fc_b\": model.fc.b,\n",
    "        \"num_layers\": len(model.encoder_layers),\n",
    "    }\n",
    "    \n",
    "    # Save all encoder layer parameters\n",
    "    for i, layer in enumerate(model.encoder_layers):\n",
    "        weights_dict[f\"encoder_{i}_self_attn_in_proj_W\"] = layer.self_attn.in_proj.W\n",
    "        weights_dict[f\"encoder_{i}_self_attn_in_proj_b\"] = layer.self_attn.in_proj.b\n",
    "        weights_dict[f\"encoder_{i}_self_attn_out_proj_W\"] = layer.self_attn.out_proj.W\n",
    "        weights_dict[f\"encoder_{i}_self_attn_out_proj_b\"] = layer.self_attn.out_proj.b\n",
    "        weights_dict[f\"encoder_{i}_linear1_W\"] = layer.linear1.W\n",
    "        weights_dict[f\"encoder_{i}_linear1_b\"] = layer.linear1.b\n",
    "        weights_dict[f\"encoder_{i}_linear2_W\"] = layer.linear2.W\n",
    "        weights_dict[f\"encoder_{i}_linear2_b\"] = layer.linear2.b\n",
    "        weights_dict[f\"encoder_{i}_norm1_gamma\"] = layer.norm1.gamma\n",
    "        weights_dict[f\"encoder_{i}_norm1_beta\"] = layer.norm1.beta\n",
    "        weights_dict[f\"encoder_{i}_norm2_gamma\"] = layer.norm2.gamma\n",
    "        weights_dict[f\"encoder_{i}_norm2_beta\"] = layer.norm2.beta\n",
    "    \n",
    "    np.savez(weights_path, **weights_dict)\n",
    "    print(f\"âœ… Saved Transformer artifacts to {out_dir}/\")\n",
    "\n",
    "\n",
    "def load_transformer_weights(weights_path, model):\n",
    "    data = np.load(weights_path)\n",
    "    \n",
    "    # Load basic parameters\n",
    "    model.emb.W[:] = data[\"emb_W\"]\n",
    "    model.fc.W[:] = data[\"fc_W\"]\n",
    "    model.fc.b[:] = data[\"fc_b\"]\n",
    "    \n",
    "    # Load encoder layer parameters\n",
    "    num_layers = int(data[\"num_layers\"])\n",
    "    for i in range(num_layers):\n",
    "        if i < len(model.encoder_layers):\n",
    "            layer = model.encoder_layers[i]\n",
    "            layer.self_attn.in_proj.W[:] = data[f\"encoder_{i}_self_attn_in_proj_W\"]\n",
    "            layer.self_attn.in_proj.b[:] = data[f\"encoder_{i}_self_attn_in_proj_b\"]\n",
    "            layer.self_attn.out_proj.W[:] = data[f\"encoder_{i}_self_attn_out_proj_W\"]\n",
    "            layer.self_attn.out_proj.b[:] = data[f\"encoder_{i}_self_attn_out_proj_b\"]\n",
    "            layer.linear1.W[:] = data[f\"encoder_{i}_linear1_W\"]\n",
    "            layer.linear1.b[:] = data[f\"encoder_{i}_linear1_b\"]\n",
    "            layer.linear2.W[:] = data[f\"encoder_{i}_linear2_W\"]\n",
    "            layer.linear2.b[:] = data[f\"encoder_{i}_linear2_b\"]\n",
    "            layer.norm1.gamma[:] = data[f\"encoder_{i}_norm1_gamma\"]\n",
    "            layer.norm1.beta[:] = data[f\"encoder_{i}_norm1_beta\"]\n",
    "            layer.norm2.gamma[:] = data[f\"encoder_{i}_norm2_gamma\"]\n",
    "            layer.norm2.beta[:] = data[f\"encoder_{i}_norm2_beta\"]\n",
    "    \n",
    "    print(\"âœ… Loaded Transformer weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer encoder-based text classifier with positional encoding and multi-layer attention\n",
    "class TransformerClassifier:\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        num_classes,\n",
    "        max_len,\n",
    "        emb_dim,\n",
    "        num_heads,\n",
    "        num_layers,\n",
    "        dim_feedforward,\n",
    "        dropout_p=0.1,\n",
    "    ):\n",
    "        self.emb = Embedding(vocab_size, emb_dim)\n",
    "        self.pos_encoding = create_positional_encoding(max_len, emb_dim)\n",
    "        self.emb_dropout = Dropout(dropout_p)\n",
    "\n",
    "        self.encoder_layers = [\n",
    "            TransformerEncoderLayer(emb_dim, num_heads, dim_feedforward, dropout_p)\n",
    "            for _ in range(num_layers)\n",
    "        ]\n",
    "\n",
    "        self.fc = Linear(emb_dim, num_classes)\n",
    "\n",
    "        self.cache = {}\n",
    "\n",
    "    def train(self):\n",
    "        self.emb_dropout.training = True\n",
    "        for layer in self.encoder_layers:\n",
    "            layer.train()\n",
    "\n",
    "    def eval(self):\n",
    "        self.emb_dropout.training = False\n",
    "        for layer in self.encoder_layers:\n",
    "            layer.eval()\n",
    "\n",
    "    def forward(self, x_ids, mask):\n",
    "        B, T = x_ids.shape\n",
    "\n",
    "        E = self.emb.forward(x_ids)\n",
    "\n",
    "        E_pos = E + self.pos_encoding[:T, :]\n",
    "        E_dropped = self.emb_dropout.forward(E_pos)\n",
    "\n",
    "        x = E_dropped\n",
    "        for layer in self.encoder_layers:\n",
    "            x = layer.forward(x, mask)\n",
    "\n",
    "        pooled = x[:, 0, :]\n",
    "\n",
    "        logits = self.fc.forward(pooled)\n",
    "\n",
    "        self.cache[\"encoder_output\"] = x\n",
    "        return logits\n",
    "\n",
    "    def backward(self, dlogits):\n",
    "        d_pooled = self.fc.backward(dlogits)\n",
    "\n",
    "        encoder_output = self.cache[\"encoder_output\"]\n",
    "        dx = np.zeros_like(encoder_output)\n",
    "        dx[:, 0, :] = d_pooled\n",
    "\n",
    "        dy = dx\n",
    "        for layer in reversed(self.encoder_layers):\n",
    "            dy = layer.backward(dy)\n",
    "\n",
    "        d_E_pos = self.emb_dropout.backward(dy)\n",
    "\n",
    "        dE = d_E_pos\n",
    "\n",
    "        self.emb.backward(dE)\n",
    "\n",
    "    def parameters_and_grads(self):\n",
    "        params = [self.emb.W, self.fc.W, self.fc.b]\n",
    "        grads = [self.emb.grad, self.fc.dW, self.fc.db]\n",
    "\n",
    "        for layer in self.encoder_layers:\n",
    "            params.extend([layer.self_attn.in_proj.W, layer.self_attn.in_proj.b])\n",
    "            params.extend([layer.self_attn.out_proj.W, layer.self_attn.out_proj.b])\n",
    "            grads.extend([layer.self_attn.in_proj.dW, layer.self_attn.in_proj.db])\n",
    "            grads.extend([layer.self_attn.out_proj.dW, layer.self_attn.out_proj.db])\n",
    "\n",
    "            params.extend(\n",
    "                [layer.linear1.W, layer.linear1.b, layer.linear2.W, layer.linear2.b]\n",
    "            )\n",
    "            grads.extend(\n",
    "                [layer.linear1.dW, layer.linear1.db, layer.linear2.dW, layer.linear2.db]\n",
    "            )\n",
    "\n",
    "            params.extend([layer.norm1.gamma, layer.norm1.beta])\n",
    "            params.extend([layer.norm2.gamma, layer.norm2.beta])\n",
    "            grads.extend([layer.norm1.dgamma, layer.norm1.dbeta])\n",
    "            grads.extend([layer.norm2.dgamma, layer.norm2.dbeta])\n",
    "\n",
    "        return params, grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preparation and preprocessing setup for all models\n",
    "tr_texts, tr_labels = read_csv_text_label(TRAIN_CSV, TEXT_COL, LABEL_COL)\n",
    "te_texts, te_labels = read_csv_text_label(TEST_CSV, TEXT_COL, LABEL_COL)\n",
    "\n",
    "label2id, id2label = build_label_map(tr_labels)\n",
    "y_tr_all = np.array([label2id[y] for y in tr_labels], dtype=np.int64)\n",
    "y_te = np.array([label2id[y] for y in te_labels], dtype=np.int64)\n",
    "\n",
    "tr_idx, va_idx = stratified_split(tr_labels, val_ratio=0.15, seed=RNG_SEED)\n",
    "tok = Tokenizer(max_vocab=30000, min_freq=1)\n",
    "tok.fit([tr_texts[i] for i in tr_idx])\n",
    "\n",
    "Xtr, Mtr = pad_sequences([tok.encode(tr_texts[i]) for i in tr_idx], MAX_LEN)\n",
    "Xva, Mva = pad_sequences([tok.encode(tr_texts[i]) for i in va_idx], MAX_LEN)\n",
    "Xte, Mte = pad_sequences([tok.encode(t) for t in te_texts], MAX_LEN)\n",
    "ytr, yva = y_tr_all[tr_idx], y_tr_all[va_idx]\n",
    "\n",
    "num_classes = len(label2id)\n",
    "vocab_size = len(tok.id2token)\n",
    "config = dict(\n",
    "    emb_dim=EMB_DIM,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    max_len=MAX_LEN,\n",
    "    vocab_size=vocab_size,\n",
    "    num_classes=num_classes,\n",
    ")\n",
    "\n",
    "\n",
    "def eval_split(model, X, M, y):\n",
    "    logits = []\n",
    "    for xb, mb, yb in batch_iter(X, M, y, batch_size=256, shuffle=False):\n",
    "        logits.append(model.forward(xb, mb))\n",
    "    logits = np.concatenate(logits, axis=0)\n",
    "    y_pred = np.argmax(logits, axis=1)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "UGJD91lYwSPk",
    "outputId": "031770a0-b937-4fd8-d441-8d5ae95af748"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class counts: [4659 5630 8429 6556 9709]\n",
      "Class weights: [1.4002557  1.1587551  0.7739697  0.9950871  0.67193234]\n",
      "  batch  547/547 | avg_loss=1.3948 | grad_norm=1.281e+00 | 82.3s\n",
      "end-epoch grad_norm=1.281e+00\n",
      "\n",
      "end-epoch grad_norm=1.281e+00\n",
      "Epoch 01 | loss=1.3948 | val_macro_f1=0.2744 | 82.3s\n",
      "  batch    1/547 | avg_loss=1.3462 | grad_norm=6.203e+00 | 0.1sEpoch 01 | loss=1.3948 | val_macro_f1=0.2744 | 82.3s\n",
      "  batch  547/547 | avg_loss=1.2630 | grad_norm=1.707e+00 | 101.3s\n",
      "end-epoch grad_norm=1.707e+00\n",
      "\n",
      "end-epoch grad_norm=1.707e+00\n",
      "Epoch 02 | loss=1.2630 | val_macro_f1=0.3219 | 101.3s\n",
      "Epoch 02 | loss=1.2630 | val_macro_f1=0.3219 | 101.3s\n",
      "  batch  547/547 | avg_loss=1.1549 | grad_norm=7.296e+00 | 79.4s\n",
      "end-epoch grad_norm=7.296e+00\n",
      "\n",
      "end-epoch grad_norm=7.296e+00\n",
      "Epoch 03 | loss=1.1549 | val_macro_f1=0.4322 | 79.4s\n",
      "  batch    1/547 | avg_loss=0.7798 | grad_norm=4.222e+00 | 0.2sEpoch 03 | loss=1.1549 | val_macro_f1=0.4322 | 79.4s\n",
      "  batch  547/547 | avg_loss=0.9263 | grad_norm=6.848e+00 | 133.8s\n",
      "end-epoch grad_norm=6.848e+00\n",
      "\n",
      "end-epoch grad_norm=6.848e+00\n",
      "Epoch 04 | loss=0.9263 | val_macro_f1=0.5433 | 133.8s\n",
      "  batch    1/547 | avg_loss=0.8094 | grad_norm=2.408e+00 | 0.1sEpoch 04 | loss=0.9263 | val_macro_f1=0.5433 | 133.8s\n",
      "  batch  547/547 | avg_loss=0.8308 | grad_norm=5.752e+00 | 188.7s\n",
      "end-epoch grad_norm=5.752e+00\n",
      "\n",
      "end-epoch grad_norm=5.752e+00\n",
      "Epoch 05 | loss=0.8308 | val_macro_f1=0.5594 | 188.7s\n",
      "Epoch 05 | loss=0.8308 | val_macro_f1=0.5594 | 188.7s\n",
      "\n",
      "Validation report:\n",
      "Macro-F1=0.5594\n",
      "Extremely Negative: P=0.440 R=0.828 F1=0.575\n",
      "Extremely Positive: P=0.505 R=0.813 F1=0.623\n",
      "Negative: P=0.528 R=0.364 F1=0.431\n",
      "Neutral: P=0.767 R=0.697 F1=0.731\n",
      "Positive: P=0.612 R=0.340 F1=0.437\n",
      "âœ… Saved artifacts to rnn_artifacts/\n",
      "\n",
      "Validation report:\n",
      "Macro-F1=0.5594\n",
      "Extremely Negative: P=0.440 R=0.828 F1=0.575\n",
      "Extremely Positive: P=0.505 R=0.813 F1=0.623\n",
      "Negative: P=0.528 R=0.364 F1=0.431\n",
      "Neutral: P=0.767 R=0.697 F1=0.731\n",
      "Positive: P=0.612 R=0.340 F1=0.437\n",
      "âœ… Saved artifacts to rnn_artifacts/\n",
      "VAL pred counts: [1546 1599 1024 1052  953]\n",
      "VAL true counts: [ 822  994 1488 1157 1713]\n",
      "TEST pred counts: [1173  894  648  570  513]\n",
      "TEST true counts: [ 592  599 1041  619  947]\n",
      "\n",
      "Validation report:\n",
      "Macro-F1=0.5594\n",
      "Extremely Negative: P=0.440 R=0.828 F1=0.575\n",
      "Extremely Positive: P=0.505 R=0.813 F1=0.623\n",
      "Negative: P=0.528 R=0.364 F1=0.431\n",
      "Neutral: P=0.767 R=0.697 F1=0.731\n",
      "Positive: P=0.612 R=0.340 F1=0.437\n",
      "\n",
      "Test Report:\n",
      "Macro-F1=0.5289\n",
      "Extremely Negative: P=0.425 R=0.841 F1=0.564\n",
      "Extremely Positive: P=0.500 R=0.746 F1=0.599\n",
      "Negative: P=0.517 R=0.322 F1=0.397\n",
      "Neutral: P=0.723 R=0.666 F1=0.693\n",
      "Positive: P=0.558 R=0.302 F1=0.392\n",
      "VAL pred counts: [1546 1599 1024 1052  953]\n",
      "VAL true counts: [ 822  994 1488 1157 1713]\n",
      "TEST pred counts: [1173  894  648  570  513]\n",
      "TEST true counts: [ 592  599 1041  619  947]\n",
      "\n",
      "Validation report:\n",
      "Macro-F1=0.5594\n",
      "Extremely Negative: P=0.440 R=0.828 F1=0.575\n",
      "Extremely Positive: P=0.505 R=0.813 F1=0.623\n",
      "Negative: P=0.528 R=0.364 F1=0.431\n",
      "Neutral: P=0.767 R=0.697 F1=0.731\n",
      "Positive: P=0.612 R=0.340 F1=0.437\n",
      "\n",
      "Test Report:\n",
      "Macro-F1=0.5289\n",
      "Extremely Negative: P=0.425 R=0.841 F1=0.564\n",
      "Extremely Positive: P=0.500 R=0.746 F1=0.599\n",
      "Negative: P=0.517 R=0.322 F1=0.397\n",
      "Neutral: P=0.723 R=0.666 F1=0.693\n",
      "Positive: P=0.558 R=0.302 F1=0.392\n"
     ]
    }
   ],
   "source": [
    "# RNN model training and evaluation pipeline\n",
    "rnn = RNNClassifier(\n",
    "    vocab_size=vocab_size,\n",
    "    num_classes=num_classes,\n",
    "    emb_dim=EMB_DIM,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    dropout=DROPOUT_P,\n",
    ")\n",
    "\n",
    "if TRAIN_MODE:\n",
    "    rnn = train_model(\n",
    "        rnn,\n",
    "        (Xtr, Mtr, ytr),\n",
    "        (Xva, Mva, yva),\n",
    "        num_classes=num_classes,\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        lr=LR,\n",
    "        weight_decay=WEIGHT_DECAY,\n",
    "        early_patience=EARLY_STOP,\n",
    "        max_grad_norm=GRAD_CLIP,\n",
    "        id2label=id2label,\n",
    "    )\n",
    "    save_rnn_artifacts(RNN_OUT_DIR, tok, label2id, config, rnn)\n",
    "else:\n",
    "    weights_path = os.path.join(RNN_OUT_DIR, \"weights_rnn_best.npz\")\n",
    "    if os.path.exists(weights_path):\n",
    "        print(\"Loading RNN weights from\", weights_path)\n",
    "        load_rnn_weights(weights_path, rnn)\n",
    "    else:\n",
    "        print(\"âš ï¸ No local weights. You can `!wget` them into\", weights_path)\n",
    "\n",
    "rnn.eval()\n",
    "\n",
    "val_pred = eval_split(rnn, Xva, Mva, yva)\n",
    "test_pred = eval_split(rnn, Xte, Mte, y_te)\n",
    "\n",
    "print(\"VAL pred counts:\", np.bincount(val_pred, minlength=num_classes))\n",
    "print(\"VAL true counts:\", np.bincount(yva, minlength=num_classes))\n",
    "print(\"TEST pred counts:\", np.bincount(test_pred, minlength=num_classes))\n",
    "print(\"TEST true counts:\", np.bincount(y_te, minlength=num_classes))\n",
    "\n",
    "print(\"\\nValidation report:\\n\" + classification_report(yva, val_pred, id2label))\n",
    "print(\"\\nTest Report:\\n\" + classification_report(y_te, test_pred, id2label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class counts: [4659 5630 8429 6556 9709]\n",
      "Class weights: [1.4002557  1.1587551  0.7739697  0.9950871  0.67193234]\n",
      "  batch  547/547 | avg_loss=1.1504 | grad_norm=1.639e+00 | 165.2s\n",
      "end-epoch grad_norm=1.639e+00\n",
      "\n",
      "end-epoch grad_norm=1.639e+00\n",
      "Epoch 01 | loss=1.1504 | val_macro_f1=0.6022 | 165.2s\n",
      "  batch    1/547 | avg_loss=0.8132 | grad_norm=1.317e+00 | 0.2sEpoch 01 | loss=1.1504 | val_macro_f1=0.6022 | 165.2s\n",
      "  batch  547/547 | avg_loss=0.7671 | grad_norm=2.132e+00 | 204.4s\n",
      "end-epoch grad_norm=2.132e+00\n",
      "\n",
      "end-epoch grad_norm=2.132e+00\n",
      "Epoch 02 | loss=0.7671 | val_macro_f1=0.6409 | 204.4s\n",
      "Epoch 02 | loss=0.7671 | val_macro_f1=0.6409 | 204.4s\n",
      "  batch  547/547 | avg_loss=0.6533 | grad_norm=1.978e+00 | 326.8s\n",
      "end-epoch grad_norm=1.978e+00\n",
      "\n",
      "end-epoch grad_norm=1.978e+00\n",
      "Epoch 03 | loss=0.6533 | val_macro_f1=0.6906 | 326.9s\n",
      "Epoch 03 | loss=0.6533 | val_macro_f1=0.6906 | 326.9s\n",
      "\n",
      "Validation report:\n",
      "Macro-F1=0.6906\n",
      "Extremely Negative: P=0.627 R=0.775 F1=0.693\n",
      "Extremely Positive: P=0.755 R=0.648 F1=0.697\n",
      "Negative: P=0.619 R=0.601 F1=0.610\n",
      "Neutral: P=0.836 R=0.719 F1=0.773\n",
      "Positive: P=0.652 R=0.710 F1=0.680\n",
      "âœ… Saved LSTM artifacts to lstm_artifacts/\n",
      "Evaluating LSTM classifier...\n",
      "\n",
      "Validation report:\n",
      "Macro-F1=0.6906\n",
      "Extremely Negative: P=0.627 R=0.775 F1=0.693\n",
      "Extremely Positive: P=0.755 R=0.648 F1=0.697\n",
      "Negative: P=0.619 R=0.601 F1=0.610\n",
      "Neutral: P=0.836 R=0.719 F1=0.773\n",
      "Positive: P=0.652 R=0.710 F1=0.680\n",
      "âœ… Saved LSTM artifacts to lstm_artifacts/\n",
      "Evaluating LSTM classifier...\n",
      "\n",
      "=== LSTM Results ===\n",
      "VAL pred counts: [1016  853 1445  995 1865]\n",
      "VAL true counts: [ 822  994 1488 1157 1713]\n",
      "TEST pred counts: [ 756  461  951  545 1085]\n",
      "TEST true counts: [ 592  599 1041  619  947]\n",
      "\n",
      "LSTM Validation Report:\n",
      "Macro-F1=0.6906\n",
      "Extremely Negative: P=0.627 R=0.775 F1=0.693\n",
      "Extremely Positive: P=0.755 R=0.648 F1=0.697\n",
      "Negative: P=0.619 R=0.601 F1=0.610\n",
      "Neutral: P=0.836 R=0.719 F1=0.773\n",
      "Positive: P=0.652 R=0.710 F1=0.680\n",
      "\n",
      "LSTM Test Report:\n",
      "Macro-F1=0.6562\n",
      "Extremely Negative: P=0.597 R=0.762 F1=0.669\n",
      "Extremely Positive: P=0.774 R=0.596 F1=0.674\n",
      "Negative: P=0.608 R=0.555 F1=0.580\n",
      "Neutral: P=0.780 R=0.687 F1=0.730\n",
      "Positive: P=0.588 R=0.674 F1=0.628\n",
      "\n",
      "=== LSTM Results ===\n",
      "VAL pred counts: [1016  853 1445  995 1865]\n",
      "VAL true counts: [ 822  994 1488 1157 1713]\n",
      "TEST pred counts: [ 756  461  951  545 1085]\n",
      "TEST true counts: [ 592  599 1041  619  947]\n",
      "\n",
      "LSTM Validation Report:\n",
      "Macro-F1=0.6906\n",
      "Extremely Negative: P=0.627 R=0.775 F1=0.693\n",
      "Extremely Positive: P=0.755 R=0.648 F1=0.697\n",
      "Negative: P=0.619 R=0.601 F1=0.610\n",
      "Neutral: P=0.836 R=0.719 F1=0.773\n",
      "Positive: P=0.652 R=0.710 F1=0.680\n",
      "\n",
      "LSTM Test Report:\n",
      "Macro-F1=0.6562\n",
      "Extremely Negative: P=0.597 R=0.762 F1=0.669\n",
      "Extremely Positive: P=0.774 R=0.596 F1=0.674\n",
      "Negative: P=0.608 R=0.555 F1=0.580\n",
      "Neutral: P=0.780 R=0.687 F1=0.730\n",
      "Positive: P=0.588 R=0.674 F1=0.628\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 3\n",
    "\n",
    "# LSTM model training and evaluation pipeline\n",
    "lstm = LSTMClassifier(\n",
    "    vocab_size=vocab_size,\n",
    "    num_classes=num_classes,\n",
    "    emb_dim=EMB_DIM,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    dropout=DROPOUT_P,\n",
    ")\n",
    "\n",
    "if TRAIN_MODE:\n",
    "    lstm = train_lstm_model(\n",
    "        lstm,\n",
    "        (Xtr, Mtr, ytr),\n",
    "        (Xva, Mva, yva),\n",
    "        num_classes=num_classes,\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        lr=LR,\n",
    "        weight_decay=WEIGHT_DECAY,\n",
    "        early_patience=EARLY_STOP,\n",
    "        max_grad_norm=GRAD_CLIP,\n",
    "        id2label=id2label,\n",
    "    )\n",
    "    save_lstm_artifacts(LSTM_OUT_DIR, tok, label2id, config, lstm)\n",
    "else:\n",
    "    weights_path = os.path.join(LSTM_OUT_DIR, \"weights_lstm_best.npz\")\n",
    "    if os.path.exists(weights_path):\n",
    "        load_lstm_weights(weights_path, lstm)\n",
    "        print(f\"Loaded LSTM weights from {weights_path}\")\n",
    "    else:\n",
    "        print(\"âš ï¸ No local LSTM weights found. Training required or download weights.\")\n",
    "\n",
    "lstm.eval()\n",
    "\n",
    "print(\"Evaluating LSTM classifier...\")\n",
    "val_pred_lstm = eval_split(lstm, Xva, Mva, yva)\n",
    "test_pred_lstm = eval_split(lstm, Xte, Mte, y_te)\n",
    "\n",
    "print(\"\\n=== LSTM Results ===\")\n",
    "print(\"VAL pred counts:\", np.bincount(val_pred_lstm, minlength=num_classes))\n",
    "print(\"VAL true counts:\", np.bincount(yva, minlength=num_classes))\n",
    "print(\"TEST pred counts:\", np.bincount(test_pred_lstm, minlength=num_classes))\n",
    "print(\"TEST true counts:\", np.bincount(y_te, minlength=num_classes))\n",
    "\n",
    "print(\n",
    "    \"\\nLSTM Validation Report:\\n\" + classification_report(yva, val_pred_lstm, id2label)\n",
    ")\n",
    "print(\"\\nLSTM Test Report:\\n\" + classification_report(y_te, test_pred_lstm, id2label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Starting Transformer model training...\n",
      "Class counts: [4659 5630 8429 6556 9709]\n",
      "Class weights: [1.4002557  1.1587551  0.7739697  0.9950871  0.67193234]\n",
      "  batch  547/547 | avg_loss=1.3452 | grad_norm=1.475e+00 | 326.4s\n",
      "end-epoch grad_norm=1.475e+00\n",
      "\n",
      "end-epoch grad_norm=1.475e+00\n",
      "Epoch 01 | loss=1.3452 | val_macro_f1=0.5283 | 326.4s\n",
      "Epoch 01 | loss=1.3452 | val_macro_f1=0.5283 | 326.4s\n",
      "  batch  547/547 | avg_loss=0.9588 | grad_norm=1.257e+00 | 319.3s\n",
      "end-epoch grad_norm=1.257e+00\n",
      "\n",
      "end-epoch grad_norm=1.257e+00\n",
      "Epoch 02 | loss=0.9588 | val_macro_f1=0.6365 | 319.3s\n",
      "Epoch 02 | loss=0.9588 | val_macro_f1=0.6365 | 319.3s\n",
      "\n",
      "Validation report:\n",
      "Macro-F1=0.6365\n",
      "Extremely Negative: P=0.488 R=0.751 F1=0.592\n",
      "Extremely Positive: P=0.739 R=0.553 F1=0.633\n",
      "Negative: P=0.684 R=0.421 F1=0.521\n",
      "Neutral: P=0.726 R=0.872 F1=0.793\n",
      "Positive: P=0.619 R=0.673 F1=0.644\n",
      "âœ… Saved Transformer artifacts to transformers_artifacts/\n",
      "\n",
      "Evaluating Transformer classifier...\n",
      "\n",
      "Validation report:\n",
      "Macro-F1=0.6365\n",
      "Extremely Negative: P=0.488 R=0.751 F1=0.592\n",
      "Extremely Positive: P=0.739 R=0.553 F1=0.633\n",
      "Negative: P=0.684 R=0.421 F1=0.521\n",
      "Neutral: P=0.726 R=0.872 F1=0.793\n",
      "Positive: P=0.619 R=0.673 F1=0.644\n",
      "âœ… Saved Transformer artifacts to transformers_artifacts/\n",
      "\n",
      "Evaluating Transformer classifier...\n",
      "\n",
      "==================== TRANSFORMER RESULTS ====================\n",
      "\n",
      "Validation Report (Transformer):\n",
      "Macro-F1=0.6365\n",
      "Extremely Negative: P=0.488 R=0.751 F1=0.592\n",
      "Extremely Positive: P=0.739 R=0.553 F1=0.633\n",
      "Negative: P=0.684 R=0.421 F1=0.521\n",
      "Neutral: P=0.726 R=0.872 F1=0.793\n",
      "Positive: P=0.619 R=0.673 F1=0.644\n",
      "\n",
      "Test Report (Transformer):\n",
      "Macro-F1=0.5892\n",
      "Extremely Negative: P=0.462 R=0.735 F1=0.567\n",
      "Extremely Positive: P=0.722 R=0.482 F1=0.579\n",
      "Negative: P=0.656 R=0.366 F1=0.470\n",
      "Neutral: P=0.649 R=0.845 F1=0.734\n",
      "Positive: P=0.562 R=0.635 F1=0.596\n",
      "\n",
      "==================== TRANSFORMER RESULTS ====================\n",
      "\n",
      "Validation Report (Transformer):\n",
      "Macro-F1=0.6365\n",
      "Extremely Negative: P=0.488 R=0.751 F1=0.592\n",
      "Extremely Positive: P=0.739 R=0.553 F1=0.633\n",
      "Negative: P=0.684 R=0.421 F1=0.521\n",
      "Neutral: P=0.726 R=0.872 F1=0.793\n",
      "Positive: P=0.619 R=0.673 F1=0.644\n",
      "\n",
      "Test Report (Transformer):\n",
      "Macro-F1=0.5892\n",
      "Extremely Negative: P=0.462 R=0.735 F1=0.567\n",
      "Extremely Positive: P=0.722 R=0.482 F1=0.579\n",
      "Negative: P=0.656 R=0.366 F1=0.470\n",
      "Neutral: P=0.649 R=0.845 F1=0.734\n",
      "Positive: P=0.562 R=0.635 F1=0.596\n"
     ]
    }
   ],
   "source": [
    "# Transformer model training and evaluation pipeline\n",
    "config[\"num_layers\"] = NUM_LAYERS\n",
    "config[\"num_heads\"] = NUM_HEADS\n",
    "config[\"dim_feedforward\"] = DIM_FEEDFORWARD\n",
    "EPOCHS = 2\n",
    "\n",
    "transformer = TransformerClassifier(\n",
    "    vocab_size=vocab_size,\n",
    "    num_classes=num_classes,\n",
    "    max_len=MAX_LEN,\n",
    "    emb_dim=EMB_DIM,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    num_heads=NUM_HEADS,\n",
    "    dim_feedforward=DIM_FEEDFORWARD,\n",
    "    dropout_p=DROPOUT_P,\n",
    ")\n",
    "\n",
    "if TRAIN_MODE:\n",
    "    print(\"ðŸš€ Starting Transformer model training...\")\n",
    "    transformer = train_transformer_model(\n",
    "        transformer,\n",
    "        (Xtr, Mtr, ytr),\n",
    "        (Xva, Mva, yva),\n",
    "        num_classes=num_classes,\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        lr=LR,\n",
    "        weight_decay=WEIGHT_DECAY,\n",
    "        early_patience=EARLY_STOP,\n",
    "        max_grad_norm=GRAD_CLIP,\n",
    "        id2label=id2label,\n",
    "    )\n",
    "    save_transformer_artifacts(TRANSFORMERS_OUT_DIR, tok, label2id, config, transformer)\n",
    "else:\n",
    "    weights_path = os.path.join(TRANSFORMERS_OUT_DIR, \"weights_transformer_best.npz\")\n",
    "    if os.path.exists(weights_path):\n",
    "        load_transformer_weights(weights_path, transformer)\n",
    "    else:\n",
    "        print(f\"âš ï¸ No local Transformer weights found at {weights_path}.\")\n",
    "\n",
    "transformer.eval()\n",
    "\n",
    "print(\"\\nEvaluating Transformer classifier...\")\n",
    "val_pred_transformer = eval_split(transformer, Xva, Mva, yva)\n",
    "test_pred_transformer = eval_split(transformer, Xte, Mte, y_te)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 20 + \" TRANSFORMER RESULTS \" + \"=\" * 20)\n",
    "print(\n",
    "    \"\\nValidation Report (Transformer):\\n\"\n",
    "    + classification_report(yva, val_pred_transformer, id2label)\n",
    ")\n",
    "print(\n",
    "    \"\\nTest Report (Transformer):\\n\"\n",
    "    + classification_report(y_te, test_pred_transformer, id2label)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Model Inference Showcase\n",
      "============================================================\n",
      "\n",
      "Found 3 models for evaluation: RNN, LSTM, Transformer\n",
      "\n",
      "------------------------------------------------------------\n",
      "Performance Comparison (Test Set Macro F1-Score)\n",
      "------------------------------------------------------------\n",
      "RNN          | Macro F1: 0.5289\n",
      "RNN          | Macro F1: 0.5289\n",
      "LSTM         | Macro F1: 0.6562\n",
      "LSTM         | Macro F1: 0.6562\n",
      "Transformer  | Macro F1: 0.5892\n",
      "\n",
      "------------------------------------------------------------\n",
      "Sample Inference on Two Random Test Examples\n",
      "------------------------------------------------------------\n",
      "\n",
      "--- Sample 1 (Index: 2619) ---\n",
      "Text:       'COVID-19.....NO ANXIETY AND NO PANIC DO NOT FEAR..... FOLLOW PHYSICAL HYGIENE.....EAT HOT  COOKED FO...'\n",
      "True Label: Extremely Negative\n",
      "Predictions:\n",
      "  - RNN          -> Extremely Negative (Conf: 0.90) [Correct]\n",
      "  - LSTM         -> Extremely Negative (Conf: 0.98) [Correct]\n",
      "  - Transformer  -> Extremely Negative (Conf: 0.80) [Correct]\n",
      "\n",
      "--- Sample 2 (Index: 456) ---\n",
      "Text:       'Just passed a queue at a petrol station and a longer one at a supermarket entrance \n",
      "Think people s...'\n",
      "True Label: Positive\n",
      "Predictions:\n",
      "  - RNN          -> Extremely Positive (Conf: 0.85) [Incorrect]\n",
      "  - LSTM         -> Positive        (Conf: 0.51) [Correct]\n",
      "  - Transformer  -> Positive        (Conf: 0.56) [Correct]\n",
      "\n",
      "============================================================\n",
      "Showcase complete.\n",
      "============================================================\n",
      "Transformer  | Macro F1: 0.5892\n",
      "\n",
      "------------------------------------------------------------\n",
      "Sample Inference on Two Random Test Examples\n",
      "------------------------------------------------------------\n",
      "\n",
      "--- Sample 1 (Index: 2619) ---\n",
      "Text:       'COVID-19.....NO ANXIETY AND NO PANIC DO NOT FEAR..... FOLLOW PHYSICAL HYGIENE.....EAT HOT  COOKED FO...'\n",
      "True Label: Extremely Negative\n",
      "Predictions:\n",
      "  - RNN          -> Extremely Negative (Conf: 0.90) [Correct]\n",
      "  - LSTM         -> Extremely Negative (Conf: 0.98) [Correct]\n",
      "  - Transformer  -> Extremely Negative (Conf: 0.80) [Correct]\n",
      "\n",
      "--- Sample 2 (Index: 456) ---\n",
      "Text:       'Just passed a queue at a petrol station and a longer one at a supermarket entrance \n",
      "Think people s...'\n",
      "True Label: Positive\n",
      "Predictions:\n",
      "  - RNN          -> Extremely Positive (Conf: 0.85) [Incorrect]\n",
      "  - LSTM         -> Positive        (Conf: 0.51) [Correct]\n",
      "  - Transformer  -> Positive        (Conf: 0.56) [Correct]\n",
      "\n",
      "============================================================\n",
      "Showcase complete.\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Model comparison and sample inference demonstration\n",
    "def extract_macro(s):\n",
    "    try:\n",
    "        line = s.splitlines()[0]\n",
    "        return float(line.split(\"=\")[1])\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "def softmax(logits):\n",
    "    if logits.ndim == 1:\n",
    "        exp_logits = np.exp(logits - np.max(logits))\n",
    "        return exp_logits / np.sum(exp_logits)\n",
    "    else:\n",
    "        exp_logits = np.exp(logits - np.max(logits, axis=1, keepdims=True))\n",
    "        return exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n",
    "\n",
    "\n",
    "def run_inference_showcase():\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Model Inference Showcase\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    available_models = {}\n",
    "    model_candidates = {\"RNN\": \"rnn\", \"LSTM\": \"lstm\", \"Transformer\": \"transformer\"}\n",
    "\n",
    "    for name, var_name in model_candidates.items():\n",
    "        if var_name in globals():\n",
    "            model_instance = globals()[var_name]\n",
    "            model_instance.eval()\n",
    "            available_models[name] = model_instance\n",
    "\n",
    "    if not available_models:\n",
    "        print(\"\\nERROR: No trained models (rnn, lstm, transformer) found.\")\n",
    "        print(\"Please ensure the models are trained and available in the global scope.\")\n",
    "        return\n",
    "\n",
    "    print(\n",
    "        f\"\\nFound {len(available_models)} models for evaluation: {', '.join(available_models.keys())}\"\n",
    "    )\n",
    "\n",
    "    print(\"\\n\" + \"-\" * 60)\n",
    "    print(\"Performance Comparison (Test Set Macro F1-Score)\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    scores = {}\n",
    "    for name, model in available_models.items():\n",
    "        test_pred = eval_split(model, Xte, Mte, y_te)\n",
    "        test_report = classification_report(y_te, test_pred, id2label)\n",
    "        test_f1 = extract_macro(test_report)\n",
    "        scores[name] = test_f1\n",
    "        print(f\"{name:<12} | Macro F1: {test_f1:.4f}\")\n",
    "\n",
    "    print(\"\\n\" + \"-\" * 60)\n",
    "    print(\"Sample Inference on Two Random Test Examples\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    if len(te_texts) < 2:\n",
    "        print(\"\\nERROR: Not enough text samples in the test set to compare.\")\n",
    "        return\n",
    "\n",
    "    sample_indices = random.sample(range(len(te_texts)), 2)\n",
    "\n",
    "    for i, idx in enumerate(sample_indices):\n",
    "        text = te_texts[idx]\n",
    "        true_label = id2label[y_te[idx]]\n",
    "\n",
    "        print(f\"\\n--- Sample {i + 1} (Index: {idx}) ---\")\n",
    "        print(f\"Text:       '{text[:100]}{'...' if len(text) > 100 else ''}'\")\n",
    "        print(f\"True Label: {true_label}\")\n",
    "        print(\"Predictions:\")\n",
    "\n",
    "        tokens = tok.encode(text)\n",
    "        x_single = np.array([tokens[:MAX_LEN]], dtype=np.int32)\n",
    "        m_single = np.ones_like(x_single, dtype=np.float32)\n",
    "        m_single[x_single == PAD] = 0.0\n",
    "\n",
    "        for name, model in available_models.items():\n",
    "            logits = model.forward(x_single, m_single)[0]\n",
    "            probabilities = softmax(logits)\n",
    "            predicted_class_idx = np.argmax(probabilities)\n",
    "            predicted_label = id2label[predicted_class_idx]\n",
    "            confidence = probabilities[predicted_class_idx]\n",
    "\n",
    "            status = \"Correct\" if predicted_label == true_label else \"Incorrect\"\n",
    "            print(\n",
    "                f\"  - {name:<12} -> {predicted_label:<15} (Conf: {confidence:.2f}) [{status}]\"\n",
    "            )\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Showcase complete.\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "\n",
    "run_inference_showcase()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
